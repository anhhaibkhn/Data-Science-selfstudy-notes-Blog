<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/" rel="alternate" type="text/html" /><updated>2022-08-12T01:48:39-05:00</updated><id>https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/feed.xml</id><title type="html">Self-study Data Science Projects notes Blog</title><subtitle>A collection of jupyter notebooks and datasets for practicing Data Science skills.</subtitle><entry><title type="html">Winning a Kaggle Competition in Python - Part 2</title><link href="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/kaggle/datacamp/eda/time%20k-fold/k-fold/stratified%20k-fold/time%20series/2022/08/03/Chapter_2_Dive_into_the_Competition.html" rel="alternate" type="text/html" title="Winning a Kaggle Competition in Python - Part 2" /><published>2022-08-03T00:00:00-05:00</published><updated>2022-08-03T00:00:00-05:00</updated><id>https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/kaggle/datacamp/eda/time%20k-fold/k-fold/stratified%20k-fold/time%20series/2022/08/03/Chapter_2_Dive_into_the_Competition</id><author><name>Hai Nguyen</name></author><category term="Kaggle" /><category term="Datacamp" /><category term="EDA" /><category term="Time K-fold" /><category term="K-fold" /><category term="Stratified K-fold" /><category term="Time Series" /><summary type="html"><![CDATA[Now that you know the basics of Kaggle competitions, you will learn how to study the specific problem at hand. You will practice EDA and get to establish correct local validation strategies. You will also learn about data leakage.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/winning_kaggle_p2.png" /><media:content medium="image" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/winning_kaggle_p2.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Winning a Kaggle Competition in Python - Part 1</title><link href="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/kaggle/datacamp/machine%20learning/python/eda/2022/08/03/Chapter_1-New-York-City-Taxi-Fare-Prediction.html" rel="alternate" type="text/html" title="Winning a Kaggle Competition in Python - Part 1" /><published>2022-08-03T00:00:00-05:00</published><updated>2022-08-03T00:00:00-05:00</updated><id>https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/kaggle/datacamp/machine%20learning/python/eda/2022/08/03/Chapter_1-New-York-City-Taxi-Fare-Prediction</id><author><name>Hai Nguyen</name></author><category term="Kaggle" /><category term="Datacamp" /><category term="Machine Learning" /><category term="Python" /><category term="EDA" /><summary type="html"><![CDATA[In this first chapter, you will get exposure to the Kaggle competition process. You will train a model and prepare a csv file ready for submission. You will learn the difference between Public and Private test splits, and how to prevent overfitting.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/winning_kaggle_p1.png" /><media:content medium="image" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/winning_kaggle_p1.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Extreme Gradient Boosting with XGBoost - Part 4 (DataCamp interactive course)</title><link href="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/python/datacamp/data%20visualization/eda/pandas/xgboost/scikit-learn/2022/05/25/Extreme-Gradient-Boosting-with-XGBoost-Part-4.html" rel="alternate" type="text/html" title="Extreme Gradient Boosting with XGBoost - Part 4 (DataCamp interactive course)" /><published>2022-05-25T00:00:00-05:00</published><updated>2022-05-25T00:00:00-05:00</updated><id>https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/python/datacamp/data%20visualization/eda/pandas/xgboost/scikit-learn/2022/05/25/Extreme-Gradient-Boosting-with-XGBoost-Part-4</id><author><name>Hai Nguyen</name></author><category term="Python" /><category term="Datacamp" /><category term="Data Visualization" /><category term="EDA" /><category term="Pandas" /><category term="XGBoost" /><category term="scikit-learn" /><summary type="html"><![CDATA[Chapter 4 - Using XGBoost in pipelines]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/xgb_part4.png" /><media:content medium="image" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/xgb_part4.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Extreme Gradient Boosting with XGBoost - Part 3 (DataCamp interactive course)</title><link href="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/python/datacamp/data%20visualization/eda/pandas/xgboost/scikit-learn/2022/05/21/Extreme-Gradient-Boosting-with-XGBoost-Part-3.html" rel="alternate" type="text/html" title="Extreme Gradient Boosting with XGBoost - Part 3 (DataCamp interactive course)" /><published>2022-05-21T00:00:00-05:00</published><updated>2022-05-21T00:00:00-05:00</updated><id>https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/python/datacamp/data%20visualization/eda/pandas/xgboost/scikit-learn/2022/05/21/Extreme-Gradient-Boosting-with-XGBoost-Part-3</id><author><name>Hai Nguyen</name></author><category term="Python" /><category term="Datacamp" /><category term="Data Visualization" /><category term="EDA" /><category term="Pandas" /><category term="XGBoost" /><category term="scikit-learn" /><summary type="html"><![CDATA[Chapter 3 - Fine-tuning your XGBoost model]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/xgb_part3.png" /><media:content medium="image" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/xgb_part3.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Extreme Gradient Boosting with XGBoost - Part 2 (DataCamp interactive course)</title><link href="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/python/datacamp/data%20visualization/eda/pandas/xgboost/scikit-learn/2022/05/20/Extreme-Gradient-Boosting-with-XGBoost-Part-2.html" rel="alternate" type="text/html" title="Extreme Gradient Boosting with XGBoost - Part 2 (DataCamp interactive course)" /><published>2022-05-20T00:00:00-05:00</published><updated>2022-05-20T00:00:00-05:00</updated><id>https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/python/datacamp/data%20visualization/eda/pandas/xgboost/scikit-learn/2022/05/20/Extreme-Gradient-Boosting-with-XGBoost-Part-2</id><author><name>Hai Nguyen</name></author><category term="Python" /><category term="Datacamp" /><category term="Data Visualization" /><category term="EDA" /><category term="Pandas" /><category term="XGBoost" /><category term="scikit-learn" /><summary type="html"><![CDATA[Chapter 2 - Regression with XGBoost]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/xgb_part2.png" /><media:content medium="image" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/xgb_part2.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>