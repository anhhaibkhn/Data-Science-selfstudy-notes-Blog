<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/" rel="alternate" type="text/html" /><updated>2022-08-16T02:51:25-05:00</updated><id>https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/feed.xml</id><title type="html">Self-study Data Science Projects notes Blog</title><subtitle>A collection of jupyter notebooks and datasets for practicing Data Science skills.</subtitle><entry><title type="html">Winning a Kaggle Competition in Python - Part 4</title><link href="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/kaggle/datacamp/modeling/blending/ensemble/grid%20search/hyperparamters%20tuning/stacking/2022/08/15/Chapter_4-Modeling.html" rel="alternate" type="text/html" title="Winning a Kaggle Competition in Python - Part 4" /><published>2022-08-15T00:00:00-05:00</published><updated>2022-08-15T00:00:00-05:00</updated><id>https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/kaggle/datacamp/modeling/blending/ensemble/grid%20search/hyperparamters%20tuning/stacking/2022/08/15/Chapter_4-Modeling</id><author><name>Hai Nguyen</name></author><category term="Kaggle" /><category term="Datacamp" /><category term="Modeling" /><category term="Blending" /><category term="Ensemble" /><category term="Grid Search" /><category term="Hyperparamters Tuning" /><category term="Stacking" /><summary type="html"><![CDATA[Modeling - Time to bring everything together and build some models! In this last chapter, you will build a base model before tuning some hyperparameters and improving your results with ensembles. You will then get some final tips and tricks to help you compete more efficiently.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/winning_kaggle_p4.png" /><media:content medium="image" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/winning_kaggle_p4.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Winning a Kaggle Competition in Python - Part 3</title><link href="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/kaggle/datacamp/feature%20engineering/encoding/cross-validation/k-fold/imputing/missing-data/2022/08/15/Chapter_3-Feature-Engineering.html" rel="alternate" type="text/html" title="Winning a Kaggle Competition in Python - Part 3" /><published>2022-08-15T00:00:00-05:00</published><updated>2022-08-15T00:00:00-05:00</updated><id>https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/kaggle/datacamp/feature%20engineering/encoding/cross-validation/k-fold/imputing/missing-data/2022/08/15/Chapter_3-Feature-Engineering</id><author><name>Hai Nguyen</name></author><category term="Kaggle" /><category term="Datacamp" /><category term="Feature Engineering" /><category term="Encoding" /><category term="Cross-Validation" /><category term="K-fold" /><category term="Imputing" /><category term="Missing-Data" /><summary type="html"><![CDATA[Feature Engineering - You will now get exposure to different types of features. You will modify existing features and create new ones. Also, you will treat the missing data accordingly.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/winning_kaggle_p3.png" /><media:content medium="image" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/winning_kaggle_p3.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Winning a Kaggle Competition in Python - Part 2</title><link href="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/kaggle/datacamp/eda/time%20k-fold/k-fold/stratified%20k-fold/time%20series/2022/08/03/Chapter_2-Dive-into-the-Competition.html" rel="alternate" type="text/html" title="Winning a Kaggle Competition in Python - Part 2" /><published>2022-08-03T00:00:00-05:00</published><updated>2022-08-03T00:00:00-05:00</updated><id>https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/kaggle/datacamp/eda/time%20k-fold/k-fold/stratified%20k-fold/time%20series/2022/08/03/Chapter_2-Dive-into-the-Competition</id><author><name>Hai Nguyen</name></author><category term="Kaggle" /><category term="Datacamp" /><category term="EDA" /><category term="Time K-fold" /><category term="K-fold" /><category term="Stratified K-fold" /><category term="Time Series" /><summary type="html"><![CDATA[Now that you know the basics of Kaggle competitions, you will learn how to study the specific problem at hand. You will practice EDA and get to establish correct local validation strategies. You will also learn about data leakage.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/winning_kaggle_p2.png" /><media:content medium="image" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/winning_kaggle_p2.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Winning a Kaggle Competition in Python - Part 1</title><link href="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/kaggle/datacamp/machine%20learning/python/eda/2022/08/03/Chapter_1-New-York-City-Taxi-Fare-Prediction.html" rel="alternate" type="text/html" title="Winning a Kaggle Competition in Python - Part 1" /><published>2022-08-03T00:00:00-05:00</published><updated>2022-08-03T00:00:00-05:00</updated><id>https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/kaggle/datacamp/machine%20learning/python/eda/2022/08/03/Chapter_1-New-York-City-Taxi-Fare-Prediction</id><author><name>Hai Nguyen</name></author><category term="Kaggle" /><category term="Datacamp" /><category term="Machine Learning" /><category term="Python" /><category term="EDA" /><summary type="html"><![CDATA[In this first chapter, you will get exposure to the Kaggle competition process. You will train a model and prepare a csv file ready for submission. You will learn the difference between Public and Private test splits, and how to prevent overfitting.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/winning_kaggle_p1.png" /><media:content medium="image" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/winning_kaggle_p1.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Extreme Gradient Boosting with XGBoost - Part 4 (DataCamp interactive course)</title><link href="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/python/datacamp/data%20visualization/eda/pandas/xgboost/scikit-learn/2022/05/25/Extreme-Gradient-Boosting-with-XGBoost-Part-4.html" rel="alternate" type="text/html" title="Extreme Gradient Boosting with XGBoost - Part 4 (DataCamp interactive course)" /><published>2022-05-25T00:00:00-05:00</published><updated>2022-05-25T00:00:00-05:00</updated><id>https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/python/datacamp/data%20visualization/eda/pandas/xgboost/scikit-learn/2022/05/25/Extreme-Gradient-Boosting-with-XGBoost-Part-4</id><author><name>Hai Nguyen</name></author><category term="Python" /><category term="Datacamp" /><category term="Data Visualization" /><category term="EDA" /><category term="Pandas" /><category term="XGBoost" /><category term="scikit-learn" /><summary type="html"><![CDATA[Chapter 4 - Using XGBoost in pipelines]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/xgb_part4.png" /><media:content medium="image" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/xgb_part4.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>