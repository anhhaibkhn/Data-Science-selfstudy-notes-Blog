<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/" rel="alternate" type="text/html" /><updated>2022-08-22T02:33:28-05:00</updated><id>https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/feed.xml</id><title type="html">Self-study Data Science Projects notes Blog</title><subtitle>A collection of jupyter notebooks and datasets for practicing Data Science skills.</subtitle><entry><title type="html">Supervised Learning with scikit-learn - Part 4</title><link href="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/datacamp/machine%20learning/supervised%20learning/python/classification/overfitting/underfitting/scikit-learn/2022/08/17/Chapter_4-Preprocessing-and-Pipelines.html" rel="alternate" type="text/html" title="Supervised Learning with scikit-learn - Part 4" /><published>2022-08-17T00:00:00-05:00</published><updated>2022-08-17T00:00:00-05:00</updated><id>https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/datacamp/machine%20learning/supervised%20learning/python/classification/overfitting/underfitting/scikit-learn/2022/08/17/Chapter_4-Preprocessing-and-Pipelines</id><author><name>Hai Nguyen</name></author><category term="Datacamp" /><category term="Machine Learning" /><category term="Supervised Learning" /><category term="Python" /><category term="Classification" /><category term="Overfitting" /><category term="Underfitting" /><category term="scikit-learn" /><summary type="html"><![CDATA[Chapter 4 - Preprocessing and Pipelines]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/supervised_learning_p4.png" /><media:content medium="image" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/supervised_learning_p4.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Supervised Learning with scikit-learn - Part 3</title><link href="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/datacamp/machine%20learning/supervised%20learning/python/classification/overfitting/underfitting/scikit-learn/2022/08/17/Chapter_3-Fine-Tuning-Your-Model.html" rel="alternate" type="text/html" title="Supervised Learning with scikit-learn - Part 3" /><published>2022-08-17T00:00:00-05:00</published><updated>2022-08-17T00:00:00-05:00</updated><id>https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/datacamp/machine%20learning/supervised%20learning/python/classification/overfitting/underfitting/scikit-learn/2022/08/17/Chapter_3-Fine-Tuning-Your-Model</id><author><name>Hai Nguyen</name></author><category term="Datacamp" /><category term="Machine Learning" /><category term="Supervised Learning" /><category term="Python" /><category term="Classification" /><category term="Overfitting" /><category term="Underfitting" /><category term="scikit-learn" /><summary type="html"><![CDATA[Chapter 3 - How good is your model?]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/supervised_learning_p3.png" /><media:content medium="image" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/supervised_learning_p3.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Supervised Learning with scikit-learn - Part 2</title><link href="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/datacamp/machine%20learning/supervised%20learning/python/classification/overfitting/underfitting/scikit-learn/2022/08/17/Chapter_2-Regression.html" rel="alternate" type="text/html" title="Supervised Learning with scikit-learn - Part 2" /><published>2022-08-17T00:00:00-05:00</published><updated>2022-08-17T00:00:00-05:00</updated><id>https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/datacamp/machine%20learning/supervised%20learning/python/classification/overfitting/underfitting/scikit-learn/2022/08/17/Chapter_2-Regression</id><author><name>Hai Nguyen</name></author><category term="Datacamp" /><category term="Machine Learning" /><category term="Supervised Learning" /><category term="Python" /><category term="Classification" /><category term="Overfitting" /><category term="Underfitting" /><category term="scikit-learn" /><summary type="html"><![CDATA[Chapter 2 - Regression]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/supervised_learning_p2.png" /><media:content medium="image" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/supervised_learning_p2.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Supervised Learning with scikit-learn - Part 1</title><link href="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/datacamp/machine%20learning/supervised%20learning/python/classification/overfitting/underfitting/scikit-learn/2022/08/17/Chapter_1-Classification.html" rel="alternate" type="text/html" title="Supervised Learning with scikit-learn - Part 1" /><published>2022-08-17T00:00:00-05:00</published><updated>2022-08-17T00:00:00-05:00</updated><id>https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/datacamp/machine%20learning/supervised%20learning/python/classification/overfitting/underfitting/scikit-learn/2022/08/17/Chapter_1-Classification</id><author><name>Hai Nguyen</name></author><category term="Datacamp" /><category term="Machine Learning" /><category term="Supervised Learning" /><category term="Python" /><category term="Classification" /><category term="Overfitting" /><category term="Underfitting" /><category term="scikit-learn" /><summary type="html"><![CDATA[Chapter 1 - Classification]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/supervised_learning_p1.png" /><media:content medium="image" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/supervised_learning_p1.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Winning a Kaggle Competition in Python - Part 3</title><link href="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/kaggle/datacamp/feature%20engineering/encoding/cross-validation/k-fold/imputing/missing-data/2022/08/15/Chapter_3-Feature-Engineering.html" rel="alternate" type="text/html" title="Winning a Kaggle Competition in Python - Part 3" /><published>2022-08-15T00:00:00-05:00</published><updated>2022-08-15T00:00:00-05:00</updated><id>https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/kaggle/datacamp/feature%20engineering/encoding/cross-validation/k-fold/imputing/missing-data/2022/08/15/Chapter_3-Feature-Engineering</id><author><name>Hai Nguyen</name></author><category term="Kaggle" /><category term="Datacamp" /><category term="Feature Engineering" /><category term="Encoding" /><category term="Cross-Validation" /><category term="K-fold" /><category term="Imputing" /><category term="Missing-Data" /><summary type="html"><![CDATA[Feature Engineering - You will now get exposure to different types of features. You will modify existing features and create new ones. Also, you will treat the missing data accordingly.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/winning_kaggle_p3.png" /><media:content medium="image" url="https://anhhaibkhn.github.io/Data-Science-selfstudy-notes-Blog/images/winning_kaggle_p3.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>