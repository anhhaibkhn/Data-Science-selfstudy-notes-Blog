{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Gradient Boosting with XGBoost - Part 3 (DataCamp interactive course)\n",
    "\n",
    "> Chapter 3 - Fine-tuning your XGBoost model\n",
    "    \n",
    "PREREQUISITES: Supervised Learning with scikit-learn, Case Study: School Budgeting with Machine Learning in Python.\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Hai Nguyen\n",
    "- categories: [Python, Datacamp, Data Visualization, EDA, Pandas, XGBoost, scikit-learn]\n",
    "- image: images/xgb_part3.png\n",
    "- hide: false\n",
    "- search_exclude: true\n",
    "- metadata_key1: metadata_value1\n",
    "- metadata_key2: metadata_value2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Download Datasets and Presentation slides for this post HERE**](https://github.com/anhhaibkhn/Data-Science-selfstudy-notes-Blog/tree/master/_notebooks/Extreme%20Gradient%20Boosting%20with%20XGBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> This chapter will teach you how to make your XGBoost models as performant as possible. You'll learn about the variety of parameters that can be adjusted to alter the behavior of XGBoost and how to tune them efficiently so that you can supercharge the performance of your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why tune your model ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>Remodeled</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>...</th>\n",
       "      <th>HouseStyle_1.5Unf</th>\n",
       "      <th>HouseStyle_1Story</th>\n",
       "      <th>HouseStyle_2.5Fin</th>\n",
       "      <th>HouseStyle_2.5Unf</th>\n",
       "      <th>HouseStyle_2Story</th>\n",
       "      <th>HouseStyle_SFoyer</th>\n",
       "      <th>HouseStyle_SLvl</th>\n",
       "      <th>PavedDrive_P</th>\n",
       "      <th>PavedDrive_Y</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>56.897260</td>\n",
       "      <td>57.623288</td>\n",
       "      <td>10516.828082</td>\n",
       "      <td>6.099315</td>\n",
       "      <td>5.575342</td>\n",
       "      <td>1971.267808</td>\n",
       "      <td>0.476712</td>\n",
       "      <td>1515.463699</td>\n",
       "      <td>0.425342</td>\n",
       "      <td>0.057534</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009589</td>\n",
       "      <td>0.497260</td>\n",
       "      <td>0.005479</td>\n",
       "      <td>0.007534</td>\n",
       "      <td>0.304795</td>\n",
       "      <td>0.025342</td>\n",
       "      <td>0.044521</td>\n",
       "      <td>0.020548</td>\n",
       "      <td>0.917808</td>\n",
       "      <td>180921.195890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>42.300571</td>\n",
       "      <td>34.664304</td>\n",
       "      <td>9981.264932</td>\n",
       "      <td>1.382997</td>\n",
       "      <td>1.112799</td>\n",
       "      <td>30.202904</td>\n",
       "      <td>0.499629</td>\n",
       "      <td>525.480383</td>\n",
       "      <td>0.518911</td>\n",
       "      <td>0.238753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097486</td>\n",
       "      <td>0.500164</td>\n",
       "      <td>0.073846</td>\n",
       "      <td>0.086502</td>\n",
       "      <td>0.460478</td>\n",
       "      <td>0.157217</td>\n",
       "      <td>0.206319</td>\n",
       "      <td>0.141914</td>\n",
       "      <td>0.274751</td>\n",
       "      <td>79442.502883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>7553.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1954.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1129.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>129975.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>9478.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1973.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1464.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>163000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>11601.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1776.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>214000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>190.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>215245.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5642.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>755000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        MSSubClass  LotFrontage        LotArea  OverallQual  OverallCond    YearBuilt    Remodeled    GrLivArea  BsmtFullBath  BsmtHalfBath  ...  HouseStyle_1.5Unf  HouseStyle_1Story  HouseStyle_2.5Fin  HouseStyle_2.5Unf  HouseStyle_2Story  HouseStyle_SFoyer  HouseStyle_SLvl  PavedDrive_P  PavedDrive_Y      SalePrice\n",
       "count  1460.000000  1460.000000    1460.000000  1460.000000  1460.000000  1460.000000  1460.000000  1460.000000   1460.000000   1460.000000  ...        1460.000000        1460.000000        1460.000000        1460.000000        1460.000000        1460.000000      1460.000000   1460.000000   1460.000000    1460.000000\n",
       "mean     56.897260    57.623288   10516.828082     6.099315     5.575342  1971.267808     0.476712  1515.463699      0.425342      0.057534  ...           0.009589           0.497260           0.005479           0.007534           0.304795           0.025342         0.044521      0.020548      0.917808  180921.195890\n",
       "std      42.300571    34.664304    9981.264932     1.382997     1.112799    30.202904     0.499629   525.480383      0.518911      0.238753  ...           0.097486           0.500164           0.073846           0.086502           0.460478           0.157217         0.206319      0.141914      0.274751   79442.502883\n",
       "min      20.000000     0.000000    1300.000000     1.000000     1.000000  1872.000000     0.000000   334.000000      0.000000      0.000000  ...           0.000000           0.000000           0.000000           0.000000           0.000000           0.000000         0.000000      0.000000      0.000000   34900.000000\n",
       "25%      20.000000    42.000000    7553.500000     5.000000     5.000000  1954.000000     0.000000  1129.500000      0.000000      0.000000  ...           0.000000           0.000000           0.000000           0.000000           0.000000           0.000000         0.000000      0.000000      1.000000  129975.000000\n",
       "50%      50.000000    63.000000    9478.500000     6.000000     5.000000  1973.000000     0.000000  1464.000000      0.000000      0.000000  ...           0.000000           0.000000           0.000000           0.000000           0.000000           0.000000         0.000000      0.000000      1.000000  163000.000000\n",
       "75%      70.000000    79.000000   11601.500000     7.000000     6.000000  2000.000000     1.000000  1776.750000      1.000000      0.000000  ...           0.000000           1.000000           0.000000           0.000000           1.000000           0.000000         0.000000      0.000000      1.000000  214000.000000\n",
       "max     190.000000   313.000000  215245.000000    10.000000     9.000000  2010.000000     1.000000  5642.000000      3.000000      2.000000  ...           1.000000           1.000000           1.000000           1.000000           1.000000           1.000000         1.000000      1.000000      1.000000  755000.000000\n",
       "\n",
       "[8 rows x 57 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_data = pd.read_csv('datasets/ames_housing_trimmed_processed.csv')\n",
    "housing_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "-1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\nguyenngochai\\.conda\\envs\\my_conda_env\\lib\\site-packages\\pandas\\core\\indexes\\range.py:385\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 385\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_range\u001b[39m.\u001b[39;49mindex(new_key)\n\u001b[0;32m    386\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[1;31mValueError\u001b[0m: -1 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nguyenngochai\\Self_study\\Data-Science-selfstudy-notes-Blog\\_notebooks\\Extreme Gradient Boosting with XGBoost\\2022-05-21-Extreme Gradient Boosting with XGBoost-Part 3.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nguyenngochai/Self_study/Data-Science-selfstudy-notes-Blog/_notebooks/Extreme%20Gradient%20Boosting%20with%20XGBoost/2022-05-21-Extreme%20Gradient%20Boosting%20with%20XGBoost-Part%203.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# run 4 fold cross validation on untuned model params\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nguyenngochai/Self_study/Data-Science-selfstudy-notes-Blog/_notebooks/Extreme%20Gradient%20Boosting%20with%20XGBoost/2022-05-21-Extreme%20Gradient%20Boosting%20with%20XGBoost-Part%203.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m untuned_cv_results_rmse \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mcv(dtrain\u001b[39m=\u001b[39mhousing_dmatrix, params\u001b[39m=\u001b[39muntuned_params,nfold\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,metrics\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrmse\u001b[39m\u001b[39m\"\u001b[39m, as_pandas\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, seed\u001b[39m=\u001b[39m\u001b[39m123\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nguyenngochai/Self_study/Data-Science-selfstudy-notes-Blog/_notebooks/Extreme%20Gradient%20Boosting%20with%20XGBoost/2022-05-21-Extreme%20Gradient%20Boosting%20with%20XGBoost-Part%203.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m untuned_rmse \u001b[39m=\u001b[39m untuned_cv_results_rmse[\u001b[39m\"\u001b[39;49m\u001b[39mtest-rmse-mean\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mtail(\u001b[39m1\u001b[39;49m)[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nguyenngochai/Self_study/Data-Science-selfstudy-notes-Blog/_notebooks/Extreme%20Gradient%20Boosting%20with%20XGBoost/2022-05-21-Extreme%20Gradient%20Boosting%20with%20XGBoost-Part%203.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mUntuned model RMSE:\u001b[39m\u001b[39m\"\u001b[39m, untuned_cv_results_rmse[\u001b[39m\"\u001b[39m\u001b[39mtest-rmse-mean\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mtail(\u001b[39m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\nguyenngochai\\.conda\\envs\\my_conda_env\\lib\\site-packages\\pandas\\core\\series.py:958\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    955\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[0;32m    957\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> 958\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[0;32m    960\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    961\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    962\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nguyenngochai\\.conda\\envs\\my_conda_env\\lib\\site-packages\\pandas\\core\\series.py:1069\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[0;32m   1068\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1069\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[0;32m   1070\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[1;32mc:\\Users\\nguyenngochai\\.conda\\envs\\my_conda_env\\lib\\site-packages\\pandas\\core\\indexes\\range.py:387\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_range\u001b[39m.\u001b[39mindex(new_key)\n\u001b[0;32m    386\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 387\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    389\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: -1"
     ]
    }
   ],
   "source": [
    "\"\"\" Untuned model example \"\"\"\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X,y = housing_data.iloc[:, :-1], housing_data.iloc[:, -1]\n",
    "\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "untuned_params = {\"objective\": \"reg:squarederror\"}\n",
    "\n",
    "# run 4 fold cross validation on untuned model params\n",
    "untuned_cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=untuned_params,nfold=4,metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "untuned_rmse = untuned_cv_results_rmse[\"test-rmse-mean\"].tail(1)\n",
    "print(\"Untuned model RMSE:\", untuned_cv_results_rmse[\"test-rmse-mean\"].tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned model RMSE: 199    29965.411196\n",
      "Name: test-rmse-mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Tuned model example \"\"\"\n",
    "# data was loaded and prepared in the above cell\n",
    "\n",
    "tuned_params = {\"objective\": \"reg:squarederror\", 'colsample_bytree': 0.3,'learning_rate': 0.1, 'max_depth': 5}\n",
    "\n",
    "tuned_cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=tuned_params,nfold=4,num_boost_round=200, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "tuned_rmse = tuned_cv_results_rmse[\"test-rmse-mean\"].tail(1)\n",
    "print(\"Tuned model RMSE:\", tuned_cv_results_rmse[\"test-rmse-mean\"].tail(1))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9     NaN\n",
       "199   NaN\n",
       "Name: test-rmse-mean, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untuned_rmse - tuned_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is almost 1400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the number of boosting rounds\n",
    "Let's start with parameter tuning by seeing how the number of boosting rounds (number of trees you build) impacts the out-of-sample performance of your XGBoost model. You'll use **xgb.cv()** inside a for loop and build one model per num_boost_round parameter.\n",
    "\n",
    "Here, you'll continue working with the Ames housing dataset. The features are available in the array X, and the target vector is contained in y.\n",
    "\n",
    "- Instructions: \n",
    "    - Create a DMatrix called housing_dmatrix from X and y.\n",
    "    - Create a parameter dictionary called params, passing in the appropriate \"objective\" (\"reg:linear\") and \"max_depth\" (set it to 3).\n",
    "    - Iterate over num_rounds inside a for loop and perform 3-fold cross-validation. In each iteration of the loop, pass in the current number of boosting rounds (curr_num_rounds) to xgb.cv() as the argument to num_boost_round.\n",
    "    - Append the final boosting round RMSE for each cross-validated XGBoost model to the final_rmse_per_round list.\n",
    "    - num_rounds and final_rmse_per_round have been zipped and converted into a DataFrame so you can easily see how the model performs with each boosting round. Hit 'Submit Answer' to see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_boosting_rounds          rmse\n",
      "0                    5  50903.298177\n",
      "1                   10  34774.192709\n",
      "2                   15  32895.097656\n",
      "3                   20  32019.971354\n",
      "4                   50  30943.686198\n",
      "5                   75  30579.746094\n",
      "6                  100  30680.307292\n",
      "7                  200  30691.264974\n"
     ]
    }
   ],
   "source": [
    "import warnings;   warnings.filterwarnings(\"ignore\") # ignored the pandas warning\n",
    " \n",
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data= X, label = y )\n",
    "\n",
    "# Create the parameter dictionary for each tree: params \n",
    "# params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":3}\n",
    "\n",
    "# Create list of number of boosting rounds\n",
    "num_rounds = [5, 10, 15, 20, 50, 75, 100, 200]\n",
    "\n",
    "# Empty list to store final round rmse per XGBoost model\n",
    "final_rmse_per_round = []\n",
    "\n",
    "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
    "for curr_num_rounds in num_rounds:\n",
    "\n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append final round RMSE\n",
    "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
    "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated boosting round selection using early_stopping\n",
    "Now, instead of attempting to cherry pick the best possible number of boosting rounds, you can very easily have XGBoost automatically select the number of boosting rounds for you within **xgb.cv()**. This is done using a technique called **early stopping**.\n",
    "\n",
    "Early stopping works by testing the XGBoost model after every boosting round against a hold-out dataset and stopping the creation of additional boosting rounds (thereby finishing training of the model early) if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds. Here you will use the early_stopping_rounds parameter in xgb.cv() with a large possible number of boosting rounds (50). Bear in mind that if the holdout metric continuously improves up through when num_boost_rounds is reached, then early stopping does not occur.\n",
    "\n",
    "Here, the DMatrix and parameter dictionary have been created for you. Your task is to use cross-validation with early stopping. Go for it!\n",
    "\n",
    "- Instructions\n",
    "    - Perform 3-fold cross-validation with early stopping and \"rmse\" as your metric. Use 10 early stopping rounds and 50 boosting rounds. Specify a seed of 123 and make sure the output is a pandas DataFrame. Remember to specify the other parameters such as dtrain, params, and metrics.\n",
    "    - Print cv_results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45    30758.543732\n",
      "46    30729.971937\n",
      "47    30732.663173\n",
      "48    30712.241251\n",
      "49    30720.853939\n",
      "Name: test-rmse-mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation with early stopping: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix,\n",
    "                    params = params,\n",
    "                    nfold=3,\n",
    "                    num_boost_round=50,\n",
    "                    metrics='rmse',\n",
    "                    as_pandas = True,\n",
    "                    early_stopping_rounds=10,\n",
    "                    seed = 123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results[\"test-rmse-mean\"].tail())\n",
    "# print(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of XGBoost's hyperparameters\n",
    "\n",
    "### Tunable parameters in XGBoost\n",
    "\n",
    "- Common tree tunable parameters\n",
    "    - learning rate: learning rate/eta\n",
    "    - gamma: min loss reduction to create new tree split\n",
    "    - lambda: L2 reg on leaf weights\n",
    "    - alpha: L1 reg on leaf weights\n",
    "    - max_depth: max depth per tree\n",
    "    - subsample: % samples used per tree\n",
    "    - colsample_bytree: % features used per  \n",
    "    <br>\n",
    "<br>   \n",
    "- Linear tunable parameters\n",
    "    - lambda: L2 reg on weights\n",
    "    - alpha: L1 reg on weights\n",
    "    - lambda_bias: L2 reg term on bias\n",
    "    - You can also tune the number of estimators used for both base model types!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning eta\n",
    "\n",
    "It's time to practice tuning other XGBoost hyperparameters in earnest and observing their effect on model performance! You'll begin by tuning the \"eta\", also known as the learning rate.\n",
    "\n",
    "The learning rate in XGBoost is a parameter that can range between 0 and 1, with higher values of \"eta\" penalizing feature weights more strongly, causing much stronger regularization.\n",
    "\n",
    "- Instructions\n",
    "\n",
    "    - Create a list called eta_vals to store the following \"eta\" values: 0.001, 0.01, and 0.1.\n",
    "    - Iterate over your eta_vals list using a for loop.\n",
    "    - In each iteration of the for loop, set the \"eta\" key of params to be equal to curr_val. Then, perform 3-fold cross-validation with early stopping (5 rounds), 10 boosting rounds, a metric of \"rmse\", and a seed of 123. Ensure the output is a DataFrame.\n",
    "    - Append the final round RMSE to the best_rmse list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     eta      best_rmse\n",
      "0  0.001  195736.402543\n",
      "1  0.010  179932.183986\n",
      "2  0.100   79759.411808\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree (boosting round)\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":3}\n",
    "\n",
    "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
    "eta_vals = [0.001, 0.01, 0.1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the eta \n",
    "for curr_val in eta_vals:\n",
    "\n",
    "    params[\"eta\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain = housing_dmatrix,\n",
    "                        params = params,\n",
    "                        nfold = 3, \n",
    "                        num_boost_round=10,\n",
    "                        early_stopping_rounds = 5,\n",
    "                        metrics =\"rmse\",\n",
    "                        as_pandas =True,\n",
    "                        seed = 123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning max_depth\n",
    "In this exercise, your job is to tune max_depth, which is the parameter that dictates the maximum depth that each tree in a boosting round can grow to. Smaller values will lead to shallower trees, and larger values to deeper trees.\n",
    "\n",
    "- Instructions\n",
    "    - Create a list called max_depths to store the following \"max_depth\" values: 2, 5, 10, and 20.\n",
    "    - Iterate over your max_depths list using a for loop.\n",
    "    - Systematically vary \"max_depth\" in each iteration of the for loop and perform 2-fold cross-validation with early stopping (5 rounds), 10 boosting rounds, a metric of \"rmse\", and a seed of 123. Ensure the output is a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   max_depth     best_rmse\n",
      "0          2  37957.469464\n",
      "1          5  35596.599504\n",
      "2         10  36065.547345\n",
      "3         20  36739.576068\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\":\"reg:squarederror\"}\n",
    "\n",
    "# Create list of max_depth values\n",
    "max_depths = [2, 5, 10, 20]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the max_depth\n",
    "for curr_val in max_depths:\n",
    "\n",
    "    params[\"max_depth\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain= housing_dmatrix,\n",
    "                        params = params,\n",
    "                        nfold = 2,\n",
    "                        num_boost_round =10,\n",
    "                        early_stopping_rounds=5,\n",
    "                        metrics=\"rmse\",\n",
    "                        as_pandas=True,\n",
    "                        seed=123)\n",
    "    \n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning colsample_bytree\n",
    "Now, it's time to tune \"colsample_bytree\". You've already seen this if you've ever worked with scikit-learn's RandomForestClassifier or RandomForestRegressor, where it just was called max_features. In both xgboost and sklearn, this parameter (although named differently) simply specifies the fraction of features to choose from at every split in a given tree. In xgboost, colsample_bytree must be specified as a float between 0 and 1.\n",
    "\n",
    "- Instructions\n",
    "    - Create a list called colsample_bytree_vals to store the values 0.1, 0.5, 0.8, and 1.\n",
    "    - Systematically vary \"colsample_bytree\" and perform cross-validation, exactly as you did with max_depth and eta previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   colsample_bytree     best_rmse\n",
      "0               0.1  40918.116895\n",
      "1               0.5  35813.904168\n",
      "2               0.8  35995.678734\n",
      "3               1.0  35836.044343\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params={\"objective\":\"reg:squarederror\",\"max_depth\":3}\n",
    "\n",
    "# Create list of hyperparameter values: colsample_bytree_vals\n",
    "colsample_bytree_vals = [0.1, 0.5, 0.8, 1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the hyperparameter value \n",
    "for curr_val in colsample_bytree_vals:\n",
    "\n",
    "    params[\"colsample_bytree\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, \n",
    "                        params=params,\n",
    "                        nfold=2,\n",
    "                        num_boost_round=10, \n",
    "                        early_stopping_rounds=5,\n",
    "                        metrics=\"rmse\",\n",
    "                        as_pandas=True,\n",
    "                        seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of grid search and random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 100 candidates, totalling 400 fits\n",
      "Best parameters found:  {'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.45}\n",
      "Lowest RMSE found:  28528.32863427011\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Grid search: example\"\"\"\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "housing_data = pd.read_csv(\"datasets/ames_housing_trimmed_processed.csv\")\n",
    "X, y = housing_data[housing_data.columns.tolist()[:-1]], housing_data[housing_data.columns.tolist()[-1]]\n",
    "\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "gbm_param_grid = {  'learning_rate': [0.01, 0.05, 0.1, 0.5, 0.9],\n",
    "                    'n_estimators': [200],\n",
    "                    # 'subsample': [0.3, 0.5, 0.9]}\n",
    "                    'subsample':  np.arange(0.05,1.05,.05)}\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "grid_mse = GridSearchCV(estimator=gbm,param_grid=gbm_param_grid,scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "\n",
    "grid_mse.fit(X, y)\n",
    "print(\"Best parameters found: \",grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))\n",
    "\n",
    "\n",
    "\"\"\" 2m41.3s\n",
    "Fitting 4 folds for each of 100 candidates, totalling 400 fits\n",
    "Best parameters found:  {'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.45}\n",
    "Lowest RMSE found:  28528.32863427011\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 25 candidates, totalling 100 fits\n",
      "Best parameters found:  {'subsample': 0.6500000000000001, 'n_estimators': 200, 'learning_rate': 0.05}\n",
      "Lowest RMSE found:  28875.728215978015\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Random search: example\"\"\"\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "housing_data = pd.read_csv(\"datasets/ames_housing_trimmed_processed.csv\")\n",
    "X,y = housing_data[housing_data.columns.tolist()[:-1]], housing_data[housing_data.columns.tolist()[-1]]\n",
    "\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "gbm_param_grid = {  'learning_rate': np.arange(0.05,1.05,.05),\n",
    "                    'n_estimators': [200],\n",
    "                    'subsample': np.arange(0.05,1.05,.05)}\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "# try with 25 random combinations\n",
    "randomized_mse = RandomizedSearchCV(estimator=gbm, param_distributions=gbm_param_grid, n_iter=25, scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "\n",
    "randomized_mse.fit(X, y)\n",
    "print(\"Best parameters found: \",randomized_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
       "       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0.05,1.05,.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search with XGBoost\n",
    "Now that you've learned how to tune parameters individually with XGBoost, let's take your parameter tuning to the next level by using scikit-learn's GridSearch and RandomizedSearch capabilities with internal cross-validation using the GridSearchCV and RandomizedSearchCV functions. You will use these to find the best model exhaustively from a collection of possible parameter values across multiple parameters simultaneously. Let's get to work, starting with GridSearchCV!\n",
    "\n",
    "- Instructions\n",
    "    - Create a parameter grid called gbm_param_grid that contains a list of \"colsample_bytree\" values (0.3, 0.7), a list with a single value for \"n_estimators\" (50), and a list of 2 \"max_depth\" (2, 5) values.\n",
    "    - Instantiate an XGBRegressor object called gbm.\n",
    "    - Create a GridSearchCV object called grid_mse, passing in: the parameter grid to param_grid, the XGBRegressor to estimator, \"neg_mean_squared_error\" to scoring, and 4 to cv. - Also specify verbose=1 so you can better understand the output.\n",
    "    - Fit the GridSearchCV object to X and y.\n",
    "    - Print the best parameter values and lowest RMSE, using the .best_params_ and .best_score_ attributes, respectively, of grid_mse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n",
      "Best parameters found:  {'colsample_bytree': 0.3, 'max_depth': 5, 'n_estimators': 50}\n",
      "Lowest RMSE found:  28986.18703093561\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter grid: gbm_param_grid\n",
    "gbm_param_grid = {\n",
    "    'colsample_bytree': [0.3, 0.7],\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [2, 5]\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "# Perform grid search: grid_mse\n",
    "grid_mse = GridSearchCV(estimator= gbm,\n",
    "                        param_grid=gbm_param_grid,\n",
    "                        scoring= 'neg_mean_squared_error',\n",
    "                        cv =4,\n",
    "                        verbose = 1)\n",
    "\n",
    "# Fit grid_mse to the data\n",
    "grid_mse.fit(X,y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search with XGBoost\n",
    "Often, GridSearchCV can be really time consuming, so in practice, you may want to use RandomizedSearchCV instead, as you will do in this exercise. The good news is you only have to make a few modifications to your GridSearchCV code to do RandomizedSearchCV. The key difference is you have to specify a param_distributions parameter instead of a param_grid parameter.\n",
    "\n",
    "\n",
    "- Instructions\n",
    "    - Create a parameter grid called gbm_param_grid that contains a list with a single value for 'n_estimators' (25), and a list of 'max_depth' values between 2 and 11 for 'max_depth' - use range(2, 12) for this.\n",
    "    - Create a RandomizedSearchCV object called randomized_mse, passing in: the parameter grid to param_distributions, the XGBRegressor to estimator, \"neg_mean_squared_error\" to scoring, 5 to n_iter, and 4 to cv. Also specify verbose=1 so you can better understand the output.\n",
    "    - Fit the RandomizedSearchCV object to X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n",
      "Best parameters found:  {'n_estimators': 25, 'max_depth': 4}\n",
      "Lowest RMSE found:  29998.4522530019\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter grid: gbm_param_grid \n",
    "gbm_param_grid = {\n",
    "    'n_estimators': [25],\n",
    "    'max_depth': range(2,12)\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor(n_estimators=10)\n",
    "\n",
    "# Perform random search: grid_mse\n",
    "randomized_mse = RandomizedSearchCV(estimator =gbm,\n",
    "                            param_distributions = gbm_param_grid,\n",
    "                            scoring='neg_mean_squared_error',\n",
    "                            cv =4, \n",
    "                            n_iter = 5,\n",
    "                            verbose =1 )\n",
    "\n",
    "\n",
    "# Fit randomized_mse to the data\n",
    "randomized_mse.fit(X,y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", randomized_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f080ef3f7e154a5496448b61eb994fbc79c03fae547c033702ffc1b7b2a346b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('my_conda_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
