{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Gradient Boosting with XGBoost - Part 1\n",
    "(DataCamp interactive course)\n",
    "\n",
    "> Do you know the basics of supervised learning and want to use state-of-the-art models on real-world datasets? Gradient boosting is currently one of the most popular techniques for efficient modeling of tabular datasets of all sizes. XGboost is a very fast, scalable implementation of gradient boosting, with models using XGBoost regularly winning online data science competitions and being used at scale across different industries. In this course, you'll learn how to use this powerful library alongside pandas and scikit-learn to build and tune supervised learning models. You'll work with real-world datasets to solve classification and regression problems.\n",
    "    - Chapter 1: Classification with XGBoost\n",
    "    - Chapter 2: Regression with XGBoost\n",
    "    - Chapter 3: Fine-tuning your XGBoost model\n",
    "    - Chapter 4: Using XGBoost in pipelines\n",
    "    \n",
    "PREREQUISITES: Supervised Learning with scikit-learn, Case Study: School Budgeting with Machine Learning in Python.\n",
    "\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Hai Nguyen\n",
    "- categories: [Python, Datacamp, Data Visualization, EDA, Pandas, XGBoost]\n",
    "- image: images/xgb_part1.png\n",
    "- hide: false\n",
    "- search_exclude: true\n",
    "- metadata_key1: metadata_value1\n",
    "- metadata_key2: metadata_value2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Chapter 1: Classification with XGBoost\n",
    "\n",
    "This chapter will introduce you to the fundamental idea behind XGBoostâ€”boosted learners. Once you understand how XGBoost works, you'll apply it to solve a common classification problem found in industry: predicting whether a customer will stop being a customer at some point in the future.\n",
    "<br />\n",
    "\n",
    "- 1.1 Course Intro:\n",
    "    - Which of these is a classification problem?\n",
    "    - Which of these is a binary classification problem?  \n",
    "    <br />\n",
    "\n",
    "- 1.2 Introducing XGBoost\n",
    "    - XGBoost: Fit/Predict\n",
    "    - Decision trees  \n",
    "    <br />\n",
    "\n",
    "- 1.3 What is Boosting?\n",
    "    - Measuring accuracy\n",
    "    - Measuring AUC  \n",
    "    <br />\n",
    "    \n",
    "- 1.4 When should I use XGBoost?\n",
    "    - Using XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Introducing XGBosst\n",
    "\n",
    "- 1.2.1 XGBoost: Fit/Predict\n",
    "    - Import xgboost as xgb.\n",
    "    - Create training and test sets such that 20% of the data is used for testing. Use a random_state of 123.\n",
    "    - Instantiate an XGBoostClassifier as xg_cl using xgb.XGBClassifier(). Specify n_estimators to be 10 estimators and an objective of 'binary:logistic'. Do not worry about what this means just yet, you will learn about these parameters later in this course.\n",
    "    - Fit xg_cl to the training set (X_train, y_train) using the .fit() method.\n",
    "    - Predict the labels of the test set (X_test) using the .predict() method and hit 'Submit Answer' to print the accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 13 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   avg_dist                     50000 non-null  float64\n",
      " 1   avg_rating_by_driver         49799 non-null  float64\n",
      " 2   avg_rating_of_driver         41878 non-null  float64\n",
      " 3   avg_inc_price                50000 non-null  float64\n",
      " 4   inc_pct                      50000 non-null  float64\n",
      " 5   weekday_pct                  50000 non-null  float64\n",
      " 6   fancy_car_user               50000 non-null  bool   \n",
      " 7   city_Carthag                 50000 non-null  int64  \n",
      " 8   city_Harko                   50000 non-null  int64  \n",
      " 9   phone_iPhone                 50000 non-null  int64  \n",
      " 10  first_month_cat_more_1_trip  50000 non-null  int64  \n",
      " 11  first_month_cat_no_trips     50000 non-null  int64  \n",
      " 12  month_5_still_here           50000 non-null  int64  \n",
      "dtypes: bool(1), float64(6), int64(6)\n",
      "memory usage: 4.6 MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_dist</th>\n",
       "      <th>avg_rating_by_driver</th>\n",
       "      <th>avg_rating_of_driver</th>\n",
       "      <th>avg_inc_price</th>\n",
       "      <th>inc_pct</th>\n",
       "      <th>weekday_pct</th>\n",
       "      <th>fancy_car_user</th>\n",
       "      <th>city_Carthag</th>\n",
       "      <th>city_Harko</th>\n",
       "      <th>phone_iPhone</th>\n",
       "      <th>first_month_cat_more_1_trip</th>\n",
       "      <th>first_month_cat_no_trips</th>\n",
       "      <th>month_5_still_here</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.67</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.10</td>\n",
       "      <td>15.4</td>\n",
       "      <td>46.2</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.26</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.77</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.36</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.14</td>\n",
       "      <td>20.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.13</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.19</td>\n",
       "      <td>11.8</td>\n",
       "      <td>82.4</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg_dist  avg_rating_by_driver  avg_rating_of_driver  avg_inc_price  \\\n",
       "0      3.67                   5.0                   4.7           1.10   \n",
       "1      8.26                   5.0                   5.0           1.00   \n",
       "2      0.77                   5.0                   4.3           1.00   \n",
       "3      2.36                   4.9                   4.6           1.14   \n",
       "4      3.13                   4.9                   4.4           1.19   \n",
       "\n",
       "   inc_pct  weekday_pct  fancy_car_user  city_Carthag  city_Harko  \\\n",
       "0     15.4         46.2            True             0           1   \n",
       "1      0.0         50.0           False             1           0   \n",
       "2      0.0        100.0           False             1           0   \n",
       "3     20.0         80.0            True             0           1   \n",
       "4     11.8         82.4           False             0           0   \n",
       "\n",
       "   phone_iPhone  first_month_cat_more_1_trip  first_month_cat_no_trips  \\\n",
       "0             1                            1                         0   \n",
       "1             0                            0                         1   \n",
       "2             1                            1                         0   \n",
       "3             1                            1                         0   \n",
       "4             0                            1                         0   \n",
       "\n",
       "   month_5_still_here  \n",
       "0                   1  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   1  \n",
       "4                   0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# churn_data = pd.read_csv(\"datasets/churn.csv\") # This dataset is not the one that was used in the course. \n",
    "churn_data = pd.read_csv(\"datasets/churn_data.csv\")\n",
    "\n",
    "print(churn_data.info())\n",
    "churn_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.758200\n"
     ]
    }
   ],
   "source": [
    "# Import xgboost\n",
    "import xgboost as xgb \n",
    "\n",
    "# Create arrays for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBClassifier: xg_cl\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', eval_metric =\"error\", n_estimators=10, seed=123, use_label_encoder=False)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xg_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_cl.predict(X_test)\n",
    "\n",
    "# Compute the accuracy: accuracy\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "print(\"accuracy: %f\" % (accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1.2.2 Decision Tree\n",
    "\n",
    "Our task in this exercise is to make a simple decision tree using scikit-learn's DecisionTreeClassifier on the breast cancer dataset that comes pre-loaded with scikit-learn.\n",
    "\n",
    "This dataset contains numeric measurements of various dimensions of individual tumors (such as perimeter and texture) from breast biopsies and a single outcome value (the tumor is either malignant, or benign).\n",
    "\n",
    "We've preloaded the dataset of samples (measurements) into X and the target values per tumor into y. Now, you have to split the complete dataset into training and testing sets, and then train a DecisionTreeClassifier. You'll specify a parameter called max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col Unnamed: 32 has True null values\n",
      "Droping Unnamed: 32\n",
      "X.shape: (569, 30), y.shape: (569,)\n",
      "accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# load the breast cancer dataset\n",
    "df = pd.read_csv(\"datasets/breast_cancer_classification_data.csv\", index_col=0)\n",
    "diagnosis_type = {'M': 1, 'B': 0}\n",
    "df.diagnosis = [diagnosis_type[item] for item in df.diagnosis]\n",
    "\n",
    "# Droping the column has NaN\n",
    "for col in df.columns:\n",
    "    null_num = df[col].isnull().values.any()\n",
    "    if null_num:\n",
    "        print(f\"col {col} has {null_num} null values\")\n",
    "        print(f\"Droping {col}\")\n",
    "        df = df.drop(col, axis=1)\n",
    "\n",
    "X, y = df.drop(\"diagnosis\", axis=1), df.diagnosis\n",
    "print(\"X.shape: {}, y.shape: {}\".format(X.shape, y.shape))\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the classifier: dt_clf_4\n",
    "dt_clf_4 = DecisionTreeClassifier(max_depth = 4)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "dt_clf_4.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred_4\n",
    "y_pred_4 = dt_clf_4.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions: accuracy\n",
    "accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]\n",
    "print(\"accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 What is Boosting\n",
    "\n",
    "- 1.3.1 Accuracy\n",
    "\n",
    "You'll now practice using XGBoost's learning API through its baked in cross-validation capabilities. As Sergey discussed in the previous video, XGBoost gets its lauded performance and efficiency gains by utilizing its own optimized data structure for datasets called a DMatrix.\n",
    "\n",
    "In the previous exercise, the input datasets were converted into DMatrix data on the fly, but when you use the xgboost cv object, you have to first explicitly convert your data into a DMatrix. So, that's what you will do here before running cross-validation on churn_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nguyenngochai\\.conda\\envs\\my_conda_env\\lib\\site-packages\\xgboost\\data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-error-mean  train-error-std  test-error-mean  test-error-std\n",
      "0           0.28232         0.002366          0.28378        0.001932\n",
      "1           0.26951         0.001855          0.27190        0.001932\n",
      "2           0.25605         0.003213          0.25798        0.003963\n",
      "3           0.25090         0.001845          0.25434        0.003827\n",
      "4           0.24654         0.001981          0.24852        0.000934\n",
      "0.75148\n"
     ]
    }
   ],
   "source": [
    "# Create arrays for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "print(type(X))\n",
    "\n",
    "# Create the DMatrix from X and y: churn_dmatrix\n",
    "churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:logistic\", \"max_depth\":3}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, \n",
    "                  nfold=3, num_boost_round=5, \n",
    "                  metrics=\"error\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the accuracy\n",
    "print(((1-cv_results[\"test-error-mean\"]).iloc[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1.3.2 Measuring AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
      "0        0.768893       0.001544       0.767863      0.002820\n",
      "1        0.790864       0.006758       0.789156      0.006847\n",
      "2        0.815872       0.003899       0.814476      0.005997\n",
      "3        0.822959       0.002018       0.821682      0.003912\n",
      "4        0.827528       0.000769       0.826191      0.001938\n",
      "0.8261913333333334\n"
     ]
    }
   ],
   "source": [
    "# Perform cross_validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, \n",
    "                  nfold=3, num_boost_round=5, \n",
    "                  metrics=\"auc\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the AUC\n",
    "print((cv_results[\"test-auc-mean\"]).iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! An AUC of 0.84 is quite strong. As you have seen, XGBoost's learning API makes it very easy to compute any metric you may be interested in. In Chapter 3, you'll learn about techniques to fine-tune your XGBoost models to improve their performance even further. For now, it's time to learn a little about exactly when to use XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When to use XGBoost:\n",
    "    - You have a large number of training samples ( > 1000 training samples, less 100 features, number of features < number of training samples)\n",
    "    - You have a mixture of categorical and numeric features ( or just numeric features)  \n",
    "    <br/>\n",
    "- When to NOT use XGBoost:\n",
    "    - Image recognition\n",
    "    - Computer Vision\n",
    "    - NLP or understanding problems\n",
    "    - Number of training samples is significantly smaller than number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f080ef3f7e154a5496448b61eb994fbc79c03fae547c033702ffc1b7b2a346b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('my_conda_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
