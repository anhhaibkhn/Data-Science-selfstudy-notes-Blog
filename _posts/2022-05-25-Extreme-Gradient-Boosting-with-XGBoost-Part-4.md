---
keywords: fastai
description: Chapter 4 - Using XGBoost in pipelines
title: Extreme Gradient Boosting with XGBoost - Part 4 (DataCamp interactive course)
toc: false
branch: master
badges: true
comments: true
author: Hai Nguyen
categories: [Python, Datacamp, Data Visualization, EDA, Pandas, XGBoost, scikit-learn]
image: images/xgb_part4.png
hide: false
search_exclude: true
metadata_key1: metadata_value1
metadata_key2: metadata_value2
nb_path: _notebooks/Extreme Gradient Boosting with XGBoost/2022-05-25-Extreme Gradient Boosting with XGBoost-Part 4.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/Extreme Gradient Boosting with XGBoost/2022-05-25-Extreme Gradient Boosting with XGBoost-Part 4.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Chapter-4-Using-XGBoost-in-pipelines">Chapter 4 Using XGBoost in pipelines<a class="anchor-link" href="#Chapter-4-Using-XGBoost-in-pipelines"> </a></h3><p>Take your XGBoost skills to the next level by incorporating your models into two end-to-end machine learning pipelines. You'll learn how to tune the most important XGBoost hyperparameters efficiently within a pipeline, and get an introduction to some more advanced preprocessing techniques.</p>
<ul>
<li>4.1 Review of pipelines using sklearn<ul>
<li>Exploratory data analysis</li>
<li>Encoding categorical columns I: LabelEncoder</li>
<li>Encoding categorical columns II: OneHotEncoder</li>
<li>Encoding categorical columns III: DictVectorizer</li>
<li>Preprocessing within a pipeline<br>
<br/></li>
</ul>
</li>
<li>4.2 Incorporating XGBoost into pipelines<ul>
<li>Cross-validating your XGBoost model</li>
<li>Kidney disease case study I: Categorical Imputer</li>
<li>Kidney disease case study II: Feature Union</li>
<li>Kidney disease case study III: Full pipeline<br>
<br/></li>
</ul>
</li>
<li>4.3 Tuning XGBoost hyperparameters<ul>
<li>Bringing it all together</li>
<li>Final Thoughts</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><p>Data Content: Each record in the database describes a Boston suburb or town. The data was drawn from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970. The attributes are deﬁned as follows     :</p>
<ul>
<li>CRIM: per capita crime rate by town</li>
<li>ZN: proportion of residential land zoned for lots over 25,000 sq.ft.</li>
<li>INDUS: proportion of non-retail business acres per town</li>
<li>CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)</li>
<li>NOX: nitric oxides concentration (parts per 10 million)</li>
<li>RM: average number of rooms per dwelling</li>
<li>AGE: proportion of owner-occupied units built prior to 1940</li>
<li>DIS: weighted distances to ﬁve Boston employment centers</li>
<li>RAD: index of accessibility to radial highways</li>
<li>TAX: full-value property-tax rate per $10,000</li>
<li>PTRATIO: pupil-teacher ratio by town</li>
<li>B: 1000(Bk−0.63)^2 where Bk is the proportion of blacks by town</li>
<li>LSTAT: % lower status of the population</li>
<li>MEDV: Median value of owner-occupied homes in $1000s We can see that the input attributes have a mixture of units.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot; Scikit-learn pipeline example &quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;display.max_columns&#39;</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>

<span class="n">colnames</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;crime&quot;</span><span class="p">,</span><span class="s2">&quot;zone&quot;</span><span class="p">,</span><span class="s2">&quot;industry&quot;</span><span class="p">,</span><span class="s2">&quot;charles&quot;</span><span class="p">,</span><span class="s2">&quot;no&quot;</span><span class="p">,</span><span class="s2">&quot;rooms&quot;</span><span class="p">,</span><span class="s2">&quot;age&quot;</span><span class="p">,</span> <span class="s2">&quot;distance&quot;</span><span class="p">,</span><span class="s2">&quot;radial&quot;</span><span class="p">,</span><span class="s2">&quot;tax&quot;</span><span class="p">,</span><span class="s2">&quot;pupil&quot;</span><span class="p">,</span><span class="s2">&quot;aam&quot;</span><span class="p">,</span><span class="s2">&quot;lower&quot;</span><span class="p">,</span><span class="s2">&quot;med_price&quot;</span><span class="p">]</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;datasets/boston_housing.csv&quot;</span><span class="p">,</span><span class="n">skiprows</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="n">colnames</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">info</span><span class="p">())</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">rf_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span> <span class="p">(</span><span class="s2">&quot;st_scaler&quot;</span><span class="p">,</span><span class="n">StandardScaler</span><span class="p">()),</span>
                        <span class="p">(</span><span class="s2">&quot;rf_model&quot;</span><span class="p">,</span><span class="n">RandomForestRegressor</span><span class="p">())])</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">rf_pipeline</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&quot;neg_mean_squared_error&quot;</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">final_avg_rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">scores</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final RMSE:&quot;</span><span class="p">,</span> <span class="n">final_avg_rmse</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>     crime  zone  industry  charles     no  rooms   age  distance  radial    tax  pupil     aam  lower  med_price
0  0.00632  18.0      2.31        0  0.538  6.575  65.2    4.0900       1  296.0   15.3  396.90   4.98       24.0
1  0.02731   0.0      7.07        0  0.469  6.421  78.9    4.9671       2  242.0   17.8  396.90   9.14       21.6
2  0.02729   0.0      7.07        0  0.469  7.185  61.1    4.9671       2  242.0   17.8  392.83   4.03       34.7
3  0.03237   0.0      2.18        0  0.458  6.998  45.8    6.0622       3  222.0   18.7  394.63   2.94       33.4
4  0.06905   0.0      2.18        0  0.458  7.147  54.2    6.0622       3  222.0   18.7  396.90   5.33       36.2
&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 506 entries, 0 to 505
Data columns (total 14 columns):
 #   Column     Non-Null Count  Dtype  
---  ------     --------------  -----  
 0   crime      506 non-null    float64
 1   zone       506 non-null    float64
 2   industry   506 non-null    float64
 3   charles    506 non-null    int64  
 4   no         506 non-null    float64
 5   rooms      506 non-null    float64
 6   age        506 non-null    float64
 7   distance   506 non-null    float64
 8   radial     506 non-null    int64  
 9   tax        506 non-null    float64
 10  pupil      506 non-null    float64
 11  aam        506 non-null    float64
 12  lower      506 non-null    float64
 13  med_price  506 non-null    float64
dtypes: float64(12), int64(2)
memory usage: 55.5 KB
None
Final RMSE: 4.211871629016008
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Exploratory-data-analysis">Exploratory data analysis<a class="anchor-link" href="#Exploratory-data-analysis"> </a></h4><p>Before diving into the nitty gritty of pipelines and preprocessing, let's do some exploratory analysis of the original, unprocessed Ames housing dataset. When you worked with this data in previous chapters, we preprocessed it for you so you could focus on the core XGBoost concepts. In this chapter, you'll do the preprocessing yourself!</p>
<p>A smaller version of this original, unprocessed dataset has been pre-loaded into a pandas DataFrame called df. Your task is to explore df in the Shell and pick the option that is incorrect. The larger purpose of this exercise is to understand the kinds of transformations you will need to perform in order to be able to use XGBoost.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_raw</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;datasets/ames_unprocessed_data.csv&quot;</span><span class="p">,</span><span class="n">skiprows</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">df_processed</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;datasets/ames_housing_trimmed_processed.csv&quot;</span><span class="p">,</span><span class="n">skiprows</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_raw</span><span class="o">.</span><span class="n">info</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_raw</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>

<span class="n">categorical_columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df_raw</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">df_raw</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="s2">&quot;object&quot;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">categorical_columns</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 1460 entries, 0 to 1459
Data columns (total 21 columns):
 #   Column        Non-Null Count  Dtype  
---  ------        --------------  -----  
 0   MSSubClass    1460 non-null   int64  
 1   MSZoning      1460 non-null   object 
 2   LotFrontage   1201 non-null   float64
 3   LotArea       1460 non-null   int64  
 4   Neighborhood  1460 non-null   object 
 5   BldgType      1460 non-null   object 
 6   HouseStyle    1460 non-null   object 
 7   OverallQual   1460 non-null   int64  
 8   OverallCond   1460 non-null   int64  
 9   YearBuilt     1460 non-null   int64  
 10  Remodeled     1460 non-null   int64  
 11  GrLivArea     1460 non-null   int64  
 12  BsmtFullBath  1460 non-null   int64  
 13  BsmtHalfBath  1460 non-null   int64  
 14  FullBath      1460 non-null   int64  
 15  HalfBath      1460 non-null   int64  
 16  BedroomAbvGr  1460 non-null   int64  
 17  Fireplaces    1460 non-null   int64  
 18  GarageArea    1460 non-null   int64  
 19  PavedDrive    1460 non-null   object 
 20  SalePrice     1460 non-null   int64  
dtypes: float64(1), int64(15), object(5)
memory usage: 239.7+ KB
None
   MSSubClass MSZoning  LotFrontage  LotArea Neighborhood BldgType HouseStyle  OverallQual  OverallCond  YearBuilt  Remodeled  GrLivArea  BsmtFullBath  BsmtHalfBath  FullBath  HalfBath  BedroomAbvGr  Fireplaces  GarageArea PavedDrive  SalePrice
0          60       RL         65.0     8450      CollgCr     1Fam     2Story            7            5       2003          0       1710             1             0         2         1             3           0         548          Y     208500
1          20       RL         80.0     9600      Veenker     1Fam     1Story            6            8       1976          0       1262             0             1         2         0             3           1         460          Y     181500
2          60       RL         68.0    11250      CollgCr     1Fam     2Story            7            5       2001          1       1786             1             0         2         1             3           1         608          Y     223500
3          70       RL         60.0     9550      Crawfor     1Fam     2Story            7            5       1915          1       1717             1             0         1         0             3           1         642          Y     140000
4          60       RL         84.0    14260      NoRidge     1Fam     2Story            8            5       2000          0       2198             1             0         2         1             4           1         836          Y     250000
[&#39;MSZoning&#39;, &#39;Neighborhood&#39;, &#39;BldgType&#39;, &#39;HouseStyle&#39;, &#39;PavedDrive&#39;]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Encoding-categorical-columns-I:-LabelEncoder">Encoding categorical columns I: LabelEncoder<a class="anchor-link" href="#Encoding-categorical-columns-I:-LabelEncoder"> </a></h4><p>Now that you've seen what will need to be done to get the housing data ready for XGBoost, let's go through the process step-by-step.</p>
<p>First, you will need to fill in missing values - as you saw previously, the column LotFrontage has many missing values. Then, you will need to encode any categorical columns in the dataset using one-hot encoding so that they are encoded numerically. You can watch this video from Supervised Learning with scikit-learn for a refresher on the idea.</p>
<p>The data has five categorical columns: MSZoning, PavedDrive, Neighborhood, BldgType, and HouseStyle. Scikit-learn has a LabelEncoder function that converts the values in each categorical column into integers. You'll practice using this here.</p>
<ul>
<li><p>Instructions</p>
<ul>
<li>Import LabelEncoder from sklearn.preprocessing.</li>
<li>Fill in missing values in the LotFrontage column with 0 using .fillna().</li>
<li>Create a boolean mask for categorical columns. You can do this by checking for whether df.dtypes equals object.</li>
<li>Create a LabelEncoder object. You can do this in the same way you instantiate any scikit-learn estimator.</li>
<li>Encode all of the categorical columns into integers using LabelEncoder(). To do this, use the .fit_transform() method of le in the provided lambda function.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">df_raw</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="c1"># Fill missing values with 0</span>
<span class="n">df</span><span class="o">.</span><span class="n">LotFrontage</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">LotFrontage</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Create a boolean mask for categorical columns</span>
<span class="n">categorical_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">dtypes</span> <span class="o">==</span> <span class="nb">object</span><span class="p">)</span>

<span class="c1"># Get list of categorical column names</span>
<span class="n">categorical_columns</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">categorical_mask</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

<span class="c1"># Print the head of the categorical columns</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">categorical_columns</span><span class="p">]</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>

<span class="c1"># Create LabelEncoder object: le</span>
<span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="c1"># Save the enconder labels to a list. </span>
<span class="n">transform_dicts</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Apply LabelEncoder to categorical columns</span>
<span class="c1"># df[categorical_columns] = df[categorical_columns].apply(lambda x: le.fit_transform(x))</span>
<span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">categorical_columns</span><span class="p">:</span>
    <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">])</span>
    <span class="n">transform_dicts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">le</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span> <span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">le</span><span class="o">.</span><span class="n">classes_</span><span class="p">))))</span>


<span class="c1"># Print the head of the LabelEncoded categorical columns</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">categorical_columns</span><span class="p">]</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="o">*</span><span class="n">transform_dicts</span><span class="p">,</span><span class="n">sep</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>  MSZoning Neighborhood BldgType HouseStyle PavedDrive
0       RL      CollgCr     1Fam     2Story          Y
1       RL      Veenker     1Fam     1Story          Y
2       RL      CollgCr     1Fam     2Story          Y
3       RL      Crawfor     1Fam     2Story          Y
4       RL      NoRidge     1Fam     2Story          Y
   MSZoning  Neighborhood  BldgType  HouseStyle  PavedDrive
0         3             5         0           5           2
1         3            24         0           2           2
2         3             5         0           5           2
3         3             6         0           5           2
4         3            15         0           5           2
{&#39;C (all)&#39;: 0, &#39;FV&#39;: 1, &#39;RH&#39;: 2, &#39;RL&#39;: 3, &#39;RM&#39;: 4}
{&#39;Blmngtn&#39;: 0, &#39;Blueste&#39;: 1, &#39;BrDale&#39;: 2, &#39;BrkSide&#39;: 3, &#39;ClearCr&#39;: 4, &#39;CollgCr&#39;: 5, &#39;Crawfor&#39;: 6, &#39;Edwards&#39;: 7, &#39;Gilbert&#39;: 8, &#39;IDOTRR&#39;: 9, &#39;MeadowV&#39;: 10, &#39;Mitchel&#39;: 11, &#39;NAmes&#39;: 12, &#39;NPkVill&#39;: 13, &#39;NWAmes&#39;: 14, &#39;NoRidge&#39;: 15, &#39;NridgHt&#39;: 16, &#39;OldTown&#39;: 17, &#39;SWISU&#39;: 18, &#39;Sawyer&#39;: 19, &#39;SawyerW&#39;: 20, &#39;Somerst&#39;: 21, &#39;StoneBr&#39;: 22, &#39;Timber&#39;: 23, &#39;Veenker&#39;: 24}
{&#39;1Fam&#39;: 0, &#39;2fmCon&#39;: 1, &#39;Duplex&#39;: 2, &#39;Twnhs&#39;: 3, &#39;TwnhsE&#39;: 4}
{&#39;1.5Fin&#39;: 0, &#39;1.5Unf&#39;: 1, &#39;1Story&#39;: 2, &#39;2.5Fin&#39;: 3, &#39;2.5Unf&#39;: 4, &#39;2Story&#39;: 5, &#39;SFoyer&#39;: 6, &#39;SLvl&#39;: 7}
{&#39;N&#39;: 0, &#39;P&#39;: 1, &#39;Y&#39;: 2}
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">transform_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">le</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span> <span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">le</span><span class="o">.</span><span class="n">classes_</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">transform_dict</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;C (all)&#39;: 0, &#39;FV&#39;: 1, &#39;RH&#39;: 2, &#39;RL&#39;: 3, &#39;RM&#39;: 4}
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Encoding-categorical-columns-II:-OneHotEncoder">Encoding categorical columns II: OneHotEncoder<a class="anchor-link" href="#Encoding-categorical-columns-II:-OneHotEncoder"> </a></h4><p>Okay - so you have your categorical columns encoded numerically. Can you now move onto using pipelines and XGBoost? Not yet! In the categorical columns of this dataset, there is no natural ordering between the entries. As an example: Using LabelEncoder, the CollgCr Neighborhood was encoded as 5, while the Veenker Neighborhood was encoded as 24, and Crawfor as 6. Is Veenker "greater" than Crawfor and CollgCr? No - and allowing the model to assume this natural ordering may result in poor performance.</p>
<p>As a result, there is another step needed: You have to apply a one-hot encoding to create binary, or "dummy" variables. You can do this using scikit-learn's OneHotEncoder.</p>
<ul>
<li>Instructions<ul>
<li>Import OneHotEncoder from sklearn.preprocessing.</li>
<li>Instantiate a OneHotEncoder object called ohe. Specify the keyword arguments categorical_features=categorical_mask and sparse=False.</li>
<li>Using its .fit_transform() method, apply the OneHotEncoder to df and save the result as df_encoded. The output will be a NumPy array.</li>
<li>Print the first 5 rows of df_encoded, and then the shape of df as well as df_encoded to compare the difference.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">ColumnTransformer</span>

<span class="c1"># Create OneHotEncoder: ohe</span>
<span class="c1"># ohe = OneHotEncoder(categorical_features = categorical_mask, sparse = False) # -----&gt; Not working in the new version of sklearn</span>
<span class="n">ct</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">([(</span><span class="s1">&#39;my_ohe&#39;</span><span class="p">,</span> <span class="n">OneHotEncoder</span><span class="p">(),</span> <span class="n">categorical_mask</span><span class="p">)],</span> <span class="n">remainder</span><span class="o">=</span><span class="s1">&#39;passthrough&#39;</span><span class="p">)</span>
<span class="c1"># Apply OneHotEncoder to categorical columns - output is no longer a dataframe: df_encoded</span>
<span class="n">df_encoded</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="c1"># Print first 3 rows of the resulting dataset - again, this will no longer be a pandas dataframe</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_encoded</span><span class="p">[:</span><span class="mi">3</span><span class="p">,</span> <span class="p">:])</span>

<span class="c1"># Print the shape of the original DataFrame</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Print the shape of the transformed array</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_encoded</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[[0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 1.000e+00 6.000e+01 6.500e+01 8.450e+03
  7.000e+00 5.000e+00 2.003e+03 0.000e+00 1.710e+03 1.000e+00 0.000e+00
  2.000e+00 1.000e+00 3.000e+00 0.000e+00 5.480e+02 2.085e+05]
 [0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 1.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 1.000e+00 2.000e+01 8.000e+01 9.600e+03
  6.000e+00 8.000e+00 1.976e+03 0.000e+00 1.262e+03 0.000e+00 1.000e+00
  2.000e+00 0.000e+00 3.000e+00 1.000e+00 4.600e+02 1.815e+05]
 [0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 1.000e+00 6.000e+01 6.800e+01 1.125e+04
  7.000e+00 5.000e+00 2.001e+03 1.000e+00 1.786e+03 1.000e+00 0.000e+00
  2.000e+00 1.000e+00 3.000e+00 1.000e+00 6.080e+02 2.235e+05]]
(1460, 21)
(1460, 62)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Encoding-categorical-columns-III:-DictVectorizer">Encoding categorical columns III: DictVectorizer<a class="anchor-link" href="#Encoding-categorical-columns-III:-DictVectorizer"> </a></h4><p>Alright, one final trick before you dive into pipelines. The two step process you just went through - <strong>LabelEncoder</strong> followed by <strong>OneHotEncoder</strong> - can be simplified by using a <strong>DictVectorizer</strong>.</p>
<p>Using a DictVectorizer on a DataFrame that has been converted to a dictionary allows you to get label encoding as well as one-hot encoding in one go.</p>
<p>Your task is to work through this strategy in this exercise!</p>
<ul>
<li><p>Instructions</p>
<ul>
<li>Import DictVectorizer from sklearn.feature_extraction.</li>
<li>Convert df into a dictionary called df_dict using its .to_dict() method with "records" as the argument.</li>
<li>Instantiate a DictVectorizer object called dv with the keyword argument sparse=False.</li>
<li>Apply the DictVectorizer on df_dict by using its .fit_transform() method.</li>
<li>Hit 'Submit Answer' to print the resulting first five rows and the vocabulary.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction</span> <span class="kn">import</span> <span class="n">DictVectorizer</span>

<span class="c1"># USE the unprocessed data for the following steps</span>
<span class="n">df_raw</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;datasets/ames_unprocessed_data.csv&quot;</span><span class="p">,</span><span class="n">skiprows</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df_raw</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s1">&#39;/n&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">info</span><span class="p">())</span>
<span class="c1"># Convert df into a dictionary: df_dict</span>
<span class="n">df_dict</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(</span><span class="s2">&quot;records&quot;</span><span class="p">)</span>

<span class="c1"># Create the DictVectorizer object: dv</span>
<span class="n">dv</span> <span class="o">=</span> <span class="n">DictVectorizer</span><span class="p">(</span><span class="n">sparse</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>

<span class="c1"># Apply dv on df: df_encoded</span>
<span class="n">df_encoded</span> <span class="o">=</span> <span class="n">dv</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_dict</span><span class="p">)</span>

<span class="c1"># Print the resulting first five rows</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_encoded</span><span class="p">[:</span><span class="mi">5</span><span class="p">,:])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_encoded</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Print the vocabulary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dv</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 1460 entries, 0 to 1459
Data columns (total 21 columns):
 #   Column        Non-Null Count  Dtype  
---  ------        --------------  -----  
 0   MSSubClass    1460 non-null   int64  
 1   MSZoning      1460 non-null   object 
 2   LotFrontage   1201 non-null   float64
 3   LotArea       1460 non-null   int64  
 4   Neighborhood  1460 non-null   object 
 5   BldgType      1460 non-null   object 
 6   HouseStyle    1460 non-null   object 
 7   OverallQual   1460 non-null   int64  
 8   OverallCond   1460 non-null   int64  
 9   YearBuilt     1460 non-null   int64  
 10  Remodeled     1460 non-null   int64  
 11  GrLivArea     1460 non-null   int64  
 12  BsmtFullBath  1460 non-null   int64  
 13  BsmtHalfBath  1460 non-null   int64  
 14  FullBath      1460 non-null   int64  
 15  HalfBath      1460 non-null   int64  
 16  BedroomAbvGr  1460 non-null   int64  
 17  Fireplaces    1460 non-null   int64  
 18  GarageArea    1460 non-null   int64  
 19  PavedDrive    1460 non-null   object 
 20  SalePrice     1460 non-null   int64  
dtypes: float64(1), int64(15), object(5)
memory usage: 239.7+ KB
(1460, 21) /n None
[[3.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00
  0.000e+00 0.000e+00 2.000e+00 5.480e+02 1.710e+03 1.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00
  8.450e+03 6.500e+01 6.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00 7.000e+00
  0.000e+00 0.000e+00 1.000e+00 0.000e+00 2.085e+05 2.003e+03]
 [3.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  1.000e+00 1.000e+00 2.000e+00 4.600e+02 1.262e+03 0.000e+00 0.000e+00
  0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  9.600e+03 8.000e+01 2.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 8.000e+00 6.000e+00
  0.000e+00 0.000e+00 1.000e+00 0.000e+00 1.815e+05 1.976e+03]
 [3.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00
  0.000e+00 1.000e+00 2.000e+00 6.080e+02 1.786e+03 1.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00
  1.125e+04 6.800e+01 6.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00 7.000e+00
  0.000e+00 0.000e+00 1.000e+00 1.000e+00 2.235e+05 2.001e+03]
 [3.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00
  0.000e+00 1.000e+00 1.000e+00 6.420e+02 1.717e+03 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00
  9.550e+03 6.000e+01 7.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00 7.000e+00
  0.000e+00 0.000e+00 1.000e+00 1.000e+00 1.400e+05 1.915e+03]
 [4.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00
  0.000e+00 1.000e+00 2.000e+00 8.360e+02 2.198e+03 1.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00
  1.426e+04 8.400e+01 6.000e+01 0.000e+00 0.000e+00 0.000e+00 1.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00
  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00 8.000e+00
  0.000e+00 0.000e+00 1.000e+00 0.000e+00 2.500e+05 2.000e+03]]
(1460, 62)
{&#39;MSSubClass&#39;: 23, &#39;MSZoning=RL&#39;: 27, &#39;LotFrontage&#39;: 22, &#39;LotArea&#39;: 21, &#39;Neighborhood=CollgCr&#39;: 34, &#39;BldgType=1Fam&#39;: 1, &#39;HouseStyle=2Story&#39;: 18, &#39;OverallQual&#39;: 55, &#39;OverallCond&#39;: 54, &#39;YearBuilt&#39;: 61, &#39;Remodeled&#39;: 59, &#39;GrLivArea&#39;: 11, &#39;BsmtFullBath&#39;: 6, &#39;BsmtHalfBath&#39;: 7, &#39;FullBath&#39;: 9, &#39;HalfBath&#39;: 12, &#39;BedroomAbvGr&#39;: 0, &#39;Fireplaces&#39;: 8, &#39;GarageArea&#39;: 10, &#39;PavedDrive=Y&#39;: 58, &#39;SalePrice&#39;: 60, &#39;Neighborhood=Veenker&#39;: 53, &#39;HouseStyle=1Story&#39;: 15, &#39;Neighborhood=Crawfor&#39;: 35, &#39;Neighborhood=NoRidge&#39;: 44, &#39;Neighborhood=Mitchel&#39;: 40, &#39;HouseStyle=1.5Fin&#39;: 13, &#39;Neighborhood=Somerst&#39;: 50, &#39;Neighborhood=NWAmes&#39;: 43, &#39;MSZoning=RM&#39;: 28, &#39;Neighborhood=OldTown&#39;: 46, &#39;Neighborhood=BrkSide&#39;: 32, &#39;BldgType=2fmCon&#39;: 2, &#39;HouseStyle=1.5Unf&#39;: 14, &#39;Neighborhood=Sawyer&#39;: 48, &#39;Neighborhood=NridgHt&#39;: 45, &#39;Neighborhood=NAmes&#39;: 41, &#39;BldgType=Duplex&#39;: 3, &#39;Neighborhood=SawyerW&#39;: 49, &#39;Neighborhood=IDOTRR&#39;: 38, &#39;PavedDrive=N&#39;: 56, &#39;Neighborhood=MeadowV&#39;: 39, &#39;BldgType=TwnhsE&#39;: 5, &#39;MSZoning=C (all)&#39;: 24, &#39;Neighborhood=Edwards&#39;: 36, &#39;Neighborhood=Timber&#39;: 52, &#39;PavedDrive=P&#39;: 57, &#39;HouseStyle=SFoyer&#39;: 19, &#39;MSZoning=FV&#39;: 25, &#39;Neighborhood=Gilbert&#39;: 37, &#39;HouseStyle=SLvl&#39;: 20, &#39;BldgType=Twnhs&#39;: 4, &#39;Neighborhood=StoneBr&#39;: 51, &#39;HouseStyle=2.5Unf&#39;: 17, &#39;Neighborhood=ClearCr&#39;: 33, &#39;Neighborhood=NPkVill&#39;: 42, &#39;HouseStyle=2.5Fin&#39;: 16, &#39;Neighborhood=Blmngtn&#39;: 29, &#39;Neighborhood=BrDale&#39;: 31, &#39;Neighborhood=SWISU&#39;: 47, &#39;MSZoning=RH&#39;: 26, &#39;Neighborhood=Blueste&#39;: 30}
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Preprocessing-within-a-pipeline">Preprocessing within a pipeline<a class="anchor-link" href="#Preprocessing-within-a-pipeline"> </a></h4><p>Now that you've seen what steps need to be taken individually to properly process the Ames housing data, let's use the much cleaner and more succinct DictVectorizer approach and put it alongside an XGBoostRegressor inside of a scikit-learn pipeline.</p>
<ul>
<li><p>Instructions</p>
<ul>
<li>Import DictVectorizer from sklearn.feature_extraction and Pipeline from sklearn.pipeline.</li>
<li>Fill in any missing values in the LotFrontage column of X with 0.</li>
<li>Complete the steps of the pipeline with DictVectorizer(sparse=False) for "ohe_onestep" and xgb.XGBRegressor() for "xgb_model".</li>
<li>Create the pipeline using Pipeline() and steps.</li>
<li>Fit the Pipeline. Don't forget to convert X into a format that DictVectorizer understands by calling the to_dict("records") method on X.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction</span> <span class="kn">import</span> <span class="n">DictVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span> 
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="c1"># import xgb</span>
<span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="nn">xgb</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">df_raw</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="c1"># Fill LotFrontage missing values with 0</span>
<span class="n">X</span><span class="o">.</span><span class="n">LotFrontage</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">LotFrontage</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Setup the pipeline steps: steps</span>
<span class="n">steps</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;ohe_onestep&quot;</span><span class="p">,</span> <span class="n">DictVectorizer</span><span class="p">(</span><span class="n">sparse</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)),</span>
         <span class="p">(</span><span class="s2">&quot;xgb_model&quot;</span><span class="p">,</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBRegressor</span><span class="p">())]</span>

<span class="c1"># Create the pipeline: xgb_pipeline</span>
<span class="n">xgb_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="p">)</span>

<span class="c1"># # Fit the pipeline</span>
<span class="c1"># xgb_pipeline.fit(X.to_dict(&quot;records&quot;),y)</span>

<span class="n">xgb_scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">xgb_pipeline</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(</span><span class="s2">&quot;records&quot;</span><span class="p">),</span><span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&quot;neg_mean_squared_error&quot;</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">final_avg_rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">xgb_scores</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final RMSE:&quot;</span><span class="p">,</span> <span class="n">final_avg_rmse</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Final RMSE: 28282.433580247784
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Incorporating-XGBoost-into-pipelines">Incorporating XGBoost into pipelines<a class="anchor-link" href="#Incorporating-XGBoost-into-pipelines"> </a></h2><h4 id="Cross-validating-ur-XGBoost-model">Cross-validating ur XGBoost model<a class="anchor-link" href="#Cross-validating-ur-XGBoost-model"> </a></h4><p>In this exercise, you'll go one step further by using the pipeline you've created to preprocess and cross-validate your model.</p>
<ul>
<li><p>Instructions</p>
<ul>
<li>Create a pipeline called xgb_pipeline using steps.</li>
<li>Perform 10-fold cross-validation using cross_val_score(). You'll have to pass in the pipeline, X (as a dictionary, using .to_dict("records")), y, the number of folds you want to use, and scoring ("neg_mean_squared_error").</li>
<li>Print the 10-fold RMSE.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction</span> <span class="kn">import</span> <span class="n">DictVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="n">X</span><span class="o">.</span><span class="n">LotFrontage</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">LotFrontage</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Setup the pipeline steps: steps</span>
<span class="n">steps</span> <span class="o">=</span> <span class="p">[</span>   <span class="p">(</span><span class="s2">&quot;ohe_onestep&quot;</span><span class="p">,</span> <span class="n">DictVectorizer</span><span class="p">(</span><span class="n">sparse</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)),</span>
            <span class="p">(</span><span class="s2">&quot;xgb_model&quot;</span><span class="p">,</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">objective</span><span class="o">=</span><span class="s2">&quot;reg:squarederror&quot;</span><span class="p">))]</span>

<span class="c1"># Create the pipeline: xgb_pipeline</span>
<span class="n">xgb_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="p">)</span>

<span class="c1"># cross-validate the model</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">xgb_pipeline</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(</span><span class="s2">&quot;records&quot;</span><span class="p">),</span><span class="n">y</span><span class="p">,</span><span class="n">scoring</span><span class="o">=</span><span class="s2">&quot;neg_mean_squared_error&quot;</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;10-fold RMSE:&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">scores</span><span class="p">))))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>10-fold RMSE: 27683.04157118635
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Kidney-disease-case-study-I:-Categorical-Imputer">Kidney disease case study I: Categorical Imputer<a class="anchor-link" href="#Kidney-disease-case-study-I:-Categorical-Imputer"> </a></h4><p>You'll now continue your exploration of using pipelines with a dataset that requires significantly more wrangling. The chronic kidney disease dataset contains both categorical and numeric features, but contains lots of missing values. The goal here is to predict who has chronic kidney disease given various blood indicators as features.</p>
<p>As Sergey mentioned in the video, you'll be introduced to a new library, sklearn_pandas, that allows you to chain many more processing steps inside of a pipeline than are currently supported in scikit-learn. Specifically, you'll be able to impute missing categorical values directly using the Categorical_Imputer() class in sklearn_pandas, and the DataFrameMapper() class to apply any arbitrary sklearn-compatible transformer on DataFrame columns, where the resulting output can be either a NumPy array or DataFrame.</p>
<p>We've also created a transformer called a Dictifier that encapsulates converting a DataFrame using .to_dict("records") without you having to do it explicitly (and so that it works in a pipeline). Finally, we've also provided the list of feature names in kidney_feature_names, the target name in kidney_target_name, the features in X, and the target in y.</p>
<p>In this exercise, your task is to apply the CategoricalImputer to impute all of the categorical columns in the dataset. You can refer to how the numeric imputation mapper was created as a template. Notice the keyword arguments input_df=True and df_out=True? This is so that you can work with DataFrames instead of arrays. By default, the transformers are passed a numpy array of the selected columns as input, and as a result, the output of the DataFrame mapper is also an array. Scikit-learn transformers have historically been designed to work with numpy arrays, not pandas DataFrames, even though their basic indexing interfaces are similar.</p>
<ul>
<li>Instructions<ul>
<li>Apply the categorical imputer using DataFrameMapper() and CategoricalImputer(). CategoricalImputer() does not need any arguments to be passed in. The columns are contained in categorical_columns. Be sure to specify input_df=True and df_out=True, and use category_feature as your iterator variable in the list comprehension.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># import pandas as pd</span>

<span class="c1"># # data = arff.loadarff(&#39;datasets/Chronic_Kidney_Disease/chronic_kidney_disease_full.arff&#39;)</span>
<span class="c1"># # df = pd.DataFrame(data[0])</span>
<span class="c1"># # df.info()</span>


<span class="c1"># # read the kidney diasease data</span>
<span class="c1"># df = pd.read_csv(&quot;datasets/kidney_disease.csv&quot;)</span>
<span class="c1"># df = df.drop([&#39;id&#39;], axis=1)</span>
<span class="c1"># print(df[&quot;classification&quot;].unique())</span>
<span class="c1"># print(df[&quot;classification&quot;].value_counts())</span>
<span class="c1"># df[&quot;classification&quot;] = df[&quot;classification&quot;].replace([&#39;ckd\t&#39;],&#39;ckd&#39;)</span>
<span class="c1"># print(df[&quot;classification&quot;].value_counts())</span>

<span class="c1"># df[&#39;classification&#39;] = df[&#39;classification&#39;].replace([&#39;notckd&#39;,&#39;ckd&#39;],[0,1])</span>


<span class="c1"># X, y = df.iloc[:,:-1], df.iloc[:,-1]</span>

<span class="c1"># print(df.info())</span>

<span class="c1"># #check number of nulls in each column</span>
<span class="c1"># nulls_per_column = X.isnull().sum()</span>
<span class="c1"># print(nulls_per_column)</span>

<span class="c1"># # create a boolean mask for categorical columns</span>
<span class="c1"># categorical_mask = (X.dtypes == object)</span>

<span class="c1"># # get list of categorical column names</span>
<span class="c1"># categorical_columns = X.columns[categorical_mask].tolist()</span>

<span class="c1"># # get list of non-categorical column names</span>
<span class="c1"># non_categorical_columns = X.columns[~categorical_mask].tolist()</span>

<span class="c1"># # apply numeric imputer to non-categorical columns</span>
<span class="c1"># from sklearn_pandas import DataFrameMapper</span>
<span class="c1"># from sklearn.impute import SimpleImputer</span>




<span class="c1"># # Apply numeric imputer</span>
<span class="c1"># numeric_imputation_mapper = DataFrameMapper(</span>
<span class="c1">#                                             [([numeric_feature], SimpleImputer(strategy=&quot;median&quot;)) for numeric_feature in non_categorical_columns],</span>
<span class="c1">#                                             input_df=True,</span>
<span class="c1">#                                             df_out=True</span>
<span class="c1">#                                            )</span>

<span class="c1"># # Apply categorical imputer</span>
<span class="c1"># categorical_imputation_mapper = DataFrameMapper(</span>
<span class="c1">#                                                 [(category_feature, SimpleImputer(strategy=&quot;most_frequent&quot;)) for category_feature in categorical_columns],</span>
<span class="c1">#                                                 input_df=True,</span>
<span class="c1">#                                                 df_out=True</span>
<span class="c1">#                                                )</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;ckd&#39; &#39;ckd\t&#39; &#39;notckd&#39;]
ckd       248
notckd    150
ckd\t       2
Name: classification, dtype: int64
ckd       250
notckd    150
Name: classification, dtype: int64
&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 400 entries, 0 to 399
Data columns (total 25 columns):
 #   Column          Non-Null Count  Dtype  
---  ------          --------------  -----  
 0   age             391 non-null    float64
 1   bp              388 non-null    float64
 2   sg              353 non-null    float64
 3   al              354 non-null    float64
 4   su              351 non-null    float64
 5   rbc             248 non-null    object 
 6   pc              335 non-null    object 
 7   pcc             396 non-null    object 
 8   ba              396 non-null    object 
 9   bgr             356 non-null    float64
 10  bu              381 non-null    float64
 11  sc              383 non-null    float64
 12  sod             313 non-null    float64
 13  pot             312 non-null    float64
 14  hemo            348 non-null    float64
 15  pcv             330 non-null    object 
 16  wc              295 non-null    object 
 17  rc              270 non-null    object 
 18  htn             398 non-null    object 
 19  dm              398 non-null    object 
 20  cad             398 non-null    object 
 21  appet           399 non-null    object 
 22  pe              399 non-null    object 
 23  ane             399 non-null    object 
 24  classification  400 non-null    int64  
dtypes: float64(11), int64(1), object(13)
memory usage: 78.2+ KB
None
age        9
bp        12
sg        47
al        46
su        49
rbc      152
pc        65
pcc        4
ba         4
bgr       44
bu        19
sc        17
sod       87
pot       88
hemo      52
pcv       70
wc       105
rc       130
htn        2
dm         2
cad        2
appet      1
pe         1
ane        1
dtype: int64
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;datasets/chronic_kidney_X.csv&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;datasets/chronic_kidney_y.csv&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="c1">#check number of nulls in each column</span>
<span class="n">nulls_per_column</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">nulls_per_column</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">info</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="s2">&quot;htn&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="s2">&quot;htn&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">())</span>
<span class="c1"># X[&quot;htn&quot;] = X[&quot;htn&quot;].replace([&#39;ckd\t&#39;],&#39;ckd&#39;)</span>
<span class="c1"># print(y[&quot;classification&quot;].value_counts())</span>

<span class="c1"># Create a boolean mask for categorical columns</span>
<span class="n">categorical_feature_mask</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dtypes</span> <span class="o">==</span> <span class="nb">object</span>
<span class="c1"># Get list of categorical column names</span>
<span class="n">categorical_columns</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">categorical_feature_mask</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>


<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>

<span class="k">for</span> <span class="n">cat_col</span> <span class="ow">in</span> <span class="n">categorical_columns</span><span class="p">:</span>
    
    <span class="n">imp</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;most_frequent&quot;</span><span class="p">)</span>
    <span class="n">X</span><span class="p">[</span><span class="n">cat_col</span><span class="p">]</span> <span class="o">=</span> <span class="n">imp</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">[[</span><span class="n">cat_col</span><span class="p">]])</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="n">le</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">LabelEncoder</span><span class="p">()</span>
    <span class="n">X</span><span class="p">[</span><span class="n">cat_col</span><span class="p">]</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">cat_col</span><span class="p">])</span>


<span class="n">nulls_per_column</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">nulls_per_column</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>age        9
bp        12
sg        47
al        46
su        49
bgr       44
bu        19
sc        17
sod       87
pot       88
hemo      52
pcv       71
wc       106
rc       131
rbc      152
pc        65
pcc        4
ba         4
htn        2
dm         2
cad        2
appet      1
pe         1
ane        1
dtype: int64
&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 400 entries, 0 to 399
Data columns (total 24 columns):
 #   Column  Non-Null Count  Dtype  
---  ------  --------------  -----  
 0   age     391 non-null    float64
 1   bp      388 non-null    float64
 2   sg      353 non-null    float64
 3   al      354 non-null    float64
 4   su      351 non-null    float64
 5   bgr     356 non-null    float64
 6   bu      381 non-null    float64
 7   sc      383 non-null    float64
 8   sod     313 non-null    float64
 9   pot     312 non-null    float64
 10  hemo    348 non-null    float64
 11  pcv     329 non-null    float64
 12  wc      294 non-null    float64
 13  rc      269 non-null    float64
 14  rbc     248 non-null    object 
 15  pc      335 non-null    object 
 16  pcc     396 non-null    object 
 17  ba      396 non-null    object 
 18  htn     398 non-null    object 
 19  dm      398 non-null    object 
 20  cad     398 non-null    object 
 21  appet   399 non-null    object 
 22  pe      399 non-null    object 
 23  ane     399 non-null    object 
dtypes: float64(14), object(10)
memory usage: 75.1+ KB
None
[&#39;yes&#39; &#39;no&#39; nan]
no     251
yes    147
Name: htn, dtype: int64
age        9
bp        12
sg        47
al        46
su        49
bgr       44
bu        19
sc        17
sod       87
pot       88
hemo      52
pcv       71
wc       106
rc       131
rbc        0
pc         0
pcc        0
ba         0
htn        0
dm         0
cad        0
appet      0
pe         0
ane        0
dtype: int64
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="c1"># import standard scaler, one hot encoder and column transformer</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span><span class="p">,</span> <span class="n">OneHotEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">ColumnTransformer</span>

<span class="c1"># Check number of nulls in each feature columns</span>
<span class="n">nulls_per_column</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">nulls_per_column</span><span class="p">)</span>

<span class="c1"># Create a boolean mask for categorical columns</span>
<span class="n">categorical_feature_mask</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dtypes</span> <span class="o">==</span> <span class="nb">object</span>

<span class="c1"># Get list of categorical column names</span>
<span class="n">categorical_columns</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">categorical_feature_mask</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="c1"># Get list of non-categorical column names</span>
<span class="n">non_categorical_columns</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="o">~</span><span class="n">categorical_feature_mask</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="c1"># convert for transformer</span>
<span class="n">numeric_features</span><span class="p">,</span> <span class="n">categorical_features</span> <span class="o">=</span> <span class="n">non_categorical_columns</span><span class="p">,</span> <span class="n">categorical_columns</span>


<span class="n">numeric_transformer</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s2">&quot;imputer&quot;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;median&quot;</span><span class="p">)),</span> <span class="p">(</span><span class="s2">&quot;scaler&quot;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">())])</span>
<span class="n">categorical_transformer</span> <span class="o">=</span><span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s2">&quot;imputer&quot;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;most_frequent&quot;</span><span class="p">)),</span> <span class="p">(</span><span class="s2">&quot;ohe&quot;</span><span class="p">,</span> <span class="n">OneHotEncoder</span><span class="p">())])</span>

<span class="n">preprocessor</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">(</span>
    <span class="n">transformers</span><span class="o">=</span><span class="p">[</span>
        <span class="p">(</span><span class="s2">&quot;num&quot;</span><span class="p">,</span> <span class="n">numeric_transformer</span><span class="p">,</span> <span class="n">numeric_features</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="n">categorical_transformer</span><span class="p">,</span> <span class="n">categorical_features</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>age        9
bp        12
sg        47
al        46
su        49
bgr       44
bu        19
sc        17
sod       87
pot       88
hemo      52
pcv       71
wc       106
rc       131
rbc        0
pc         0
pcc        0
ba         0
htn        0
dm         0
cad        0
appet      0
pe         0
ane        0
dtype: int64
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># imputer = SimpleImputer(strategy=&#39;constant&#39;, fill_value=&#39;missing&#39;)</span>

<span class="c1"># df[&#39;rbc&#39;] = imputer.fit_transform(df[&#39;rbc&#39;].values.reshape(-1,1))[:,0]</span>

<span class="c1"># df[&#39;rbc&#39;]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">categorical_transformer</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;)),
                (&#39;ohe&#39;, OneHotEncoder())])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Kidney-disease-case-study-II:-Feature-Union">Kidney disease case study II: Feature Union<a class="anchor-link" href="#Kidney-disease-case-study-II:-Feature-Union"> </a></h4><p>Having separately imputed numeric as well as categorical columns, your task is now to use scikit-learn's FeatureUnion to concatenate their results, which are contained in two separate transformer objects - numeric_imputation_mapper, and categorical_imputation_mapper, respectively.</p>
<p>You may have already encountered FeatureUnion in Machine Learning with the Experts: School Budgets. Just like with pipelines, you have to pass it a list of (string, transformer) tuples, where the first half of each tuple is the name of the transformer.</p>
<ul>
<li><p>Instructions</p>
<ul>
<li>Import FeatureUnion from sklearn.pipeline.</li>
<li>Combine the results of numeric_imputation_mapper and categorical_imputation_mapper using FeatureUnion(), with the names "num_mapper" and "cat_mapper" respectively.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># from sklearn.pipeline import FeatureUnion</span>

<span class="c1"># # Combine the numeric and categorical transformations</span>
<span class="c1"># numeric_categorical_union = FeatureUnion([</span>
<span class="c1">#                                           (&quot;num_mapper&quot;, numeric_imputation_mapper),</span>
<span class="c1">#                                           (&quot;cat_mapper&quot;, categorical_imputation_mapper)</span>
<span class="c1">#                                          ])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Kidney-disease-case-study-III:-Full-pipeline">Kidney disease case study III: Full pipeline<a class="anchor-link" href="#Kidney-disease-case-study-III:-Full-pipeline"> </a></h4><p>It's time to piece together all of the transforms along with an XGBClassifier to build the full pipeline!</p>
<p>Besides the numeric_categorical_union that you created in the previous exercise, there are two other transforms needed: the Dictifier() transform which we created for you, and the DictVectorizer().</p>
<p>After creating the pipeline, your task is to cross-validate it to see how well it performs.</p>
<ul>
<li><p>Instructions</p>
<ul>
<li>Create the pipeline using the numeric_categorical_union, Dictifier(), and DictVectorizer(sort=False) transforms, and xgb.XGBClassifier() estimator with max_depth=3. Name the transforms "featureunion", "dictifier" "vectorizer", and the estimator "clf".</li>
<li>Perform 3-fold cross-validation on the pipeline using cross_val_score(). Pass it the pipeline, pipeline, the features, kidney_data, the outcomes, y. Also set scoring to "roc_auc" and cv to 3.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span>

<span class="c1"># Define Dictifier class to turn df into dictionary as part of pipeline</span>
<span class="k">class</span> <span class="nc">Dictifier</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>       
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">==</span> <span class="n">pd</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">frame</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">X</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(</span><span class="s2">&quot;records&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(</span><span class="s2">&quot;records&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction</span> <span class="kn">import</span> <span class="n">DictVectorizer</span>
<span class="c1"># import PIpline</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="c1"># import xgb</span>
<span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="nn">xgb</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="c1"># cross-validation</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="c1"># Create full pipeline , SET eval_metric=&#39;rmse&#39; to disable Warning, the results were unchanged</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
                     <span class="p">(</span><span class="s2">&quot;preprocessor&quot;</span><span class="p">,</span> <span class="n">preprocessor</span><span class="p">),</span>
                    <span class="c1">#  (&quot;dictifier&quot;, Dictifier()),</span>
                    <span class="c1">#  (&quot;vectorizer&quot;, DictVectorizer(sort=False)),</span>
                     <span class="p">(</span><span class="s2">&quot;clf&quot;</span><span class="p">,</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span> <span class="n">eval_metric</span><span class="o">=</span><span class="s1">&#39;rmse&#39;</span><span class="p">,</span><span class="n">use_label_encoder</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>
                    <span class="p">])</span>

<span class="c1"># Perform cross-validation</span>
<span class="n">cross_val_scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&quot;roc_auc&quot;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Print avg. AUC</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;3-fold AUC: &quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cross_val_scores</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>3-fold AUC:  0.998237712755785
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>

<span class="c1"># Create the parameter grid</span>
<span class="n">gbm_param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;clf__learning_rate&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">),</span>
    <span class="s1">&#39;clf__max_depth&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="s1">&#39;clf__n_estimators&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="p">}</span>

<span class="c1"># Perform RandomizedSearchCV</span>
<span class="n">randomized_roc_auc</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">param_distributions</span> <span class="o">=</span> <span class="n">gbm_param_grid</span> <span class="p">,</span> <span class="n">scoring</span><span class="o">=</span> <span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Fit the estimator</span>
<span class="n">randomized_roc_auc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Compute metrics</span>
<span class="nb">print</span><span class="p">(</span><span class="n">randomized_roc_auc</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">randomized_roc_auc</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Fitting 2 folds for each of 20 candidates, totalling 40 fits
0.9975466666666666
Pipeline(steps=[(&#39;preprocessor&#39;,
                 ColumnTransformer(transformers=[(&#39;num&#39;,
                                                  Pipeline(steps=[(&#39;imputer&#39;,
                                                                   SimpleImputer(strategy=&#39;median&#39;)),
                                                                  (&#39;scaler&#39;,
                                                                   StandardScaler())]),
                                                  [&#39;age&#39;, &#39;bp&#39;, &#39;sg&#39;, &#39;al&#39;,
                                                   &#39;su&#39;, &#39;bgr&#39;, &#39;bu&#39;, &#39;sc&#39;,
                                                   &#39;sod&#39;, &#39;pot&#39;, &#39;hemo&#39;, &#39;pcv&#39;,
                                                   &#39;wc&#39;, &#39;rc&#39;, &#39;rbc&#39;, &#39;pc&#39;,
                                                   &#39;pcc&#39;, &#39;ba&#39;, &#39;htn&#39;, &#39;dm&#39;,
                                                   &#39;cad&#39;, &#39;appet&#39;, &#39;pe&#39;,
                                                   &#39;ane&#39;]),
                                                 (&#39;cat&#39;,
                                                  Pipeline(steps=[(&#39;imputer&#39;,
                                                                   SimpleImputer(st...
                               importance_type=None, interaction_constraints=&#39;&#39;,
                               learning_rate=0.5, max_delta_step=0, max_depth=9,
                               min_child_weight=1, missing=nan,
                               monotone_constraints=&#39;()&#39;, n_estimators=100,
                               n_jobs=8, num_parallel_tree=1, predictor=&#39;auto&#39;,
                               random_state=0, reg_alpha=0, reg_lambda=1,
                               scale_pos_weight=1, subsample=1,
                               tree_method=&#39;exact&#39;, use_label_encoder=False,
                               validate_parameters=1, verbosity=None))])
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-intense-fg ansi-bold">The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click &lt;a href=&#39;https://aka.ms/vscodeJupyterKernelCrash&#39;&gt;here&lt;/a&gt; for more info. View Jupyter &lt;a href=&#39;command:jupyter.viewOutput&#39;&gt;log&lt;/a&gt; for further details.</span></pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

